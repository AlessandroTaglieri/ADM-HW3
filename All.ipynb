{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "f = open(\"/Users/digitalfirst/Desktop/HW3 ADM/movies3.html\")\n",
    "\n",
    "soup = BeautifulSoup(f)\n",
    "listUrl_Movies3=[]\n",
    "for link in soup.select('a'):\n",
    "    listUrl_Movies3.append(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile(index):\n",
    "\n",
    "    for index in range(len(listUrl_Movies3)):\n",
    "        \n",
    "        t2=1200\n",
    "        try:\n",
    "            t1 = random.randint(1,5)\n",
    "            time.sleep(t1)\n",
    "            url=listUrl_Movies3[index]\n",
    "            response = requests.get(url)\n",
    "            name=\"aritcle_\"\n",
    "            extension=\".html\"\n",
    "            file=\"{}{}{}\".format(name,index,extension)\n",
    "            with open(file,'wb') as f: \n",
    "                f.write(response.content)  \n",
    "\n",
    "        except response.status_code as e:\n",
    "            print(\"exception\")\n",
    "            if e==492:\n",
    "                time.sleep(t2)\n",
    "                downloadFile(index+1)\n",
    "            elif e==200:\n",
    "                soup = BeautifulSoup(listUrl_Movies3[1])\n",
    "                \n",
    "                with open(file,'w') as f: \n",
    "                    f.write(soup.text)\n",
    "                downloadFile(index+1)\n",
    "            else:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadFile(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os.path\n",
    "#define column of our dataframe\n",
    "df=pd.DataFrame(columns=['title', 'intro', 'plot','film_name','producer','director','writer','starring','music','release date','runtime','country','language','budget'])\n",
    "\n",
    "\n",
    "for index in range(len(listUrl_Movies3)):\n",
    "    \n",
    "    title=''\n",
    "    plot=''\n",
    "    intro=''\n",
    "    title_name='NA'\n",
    "    producer='NA'\n",
    "    director='NA'\n",
    "    writer='NA'\n",
    "    starring='NA'\n",
    "    music='NA'\n",
    "    release_date='NA'\n",
    "    runtime='NA'\n",
    "    country='NA'\n",
    "    language='NA'\n",
    "    budget='NA'\n",
    "    \n",
    "    \n",
    "    #define name of the file that we want to find (in my case: in the same directory)\n",
    "    name=\"aritcle_\"\n",
    "    extension=\".html\"\n",
    "    file=\"{}{}{}\".format(name,index,extension)\n",
    "    \n",
    "    #check if this file exists\n",
    "    if not os.path.isfile(file):\n",
    "        continue\n",
    "        \n",
    "    #open file   \n",
    "    response2 = open(file)\n",
    "    soup = BeautifulSoup(response2)\n",
    "    #take title.\n",
    "    title=soup.title.text.rsplit(' ', 2)[0]\n",
    "    \n",
    "    #take all p in intro(firt section)\n",
    "    if soup.find('span', attrs={'class': 'mw-headline'}):\n",
    "        \n",
    "        heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "        paragraphs = heading.find_all_previous('p')\n",
    "        for p in paragraphs: \n",
    "            intro = p.text + intro\n",
    "            \n",
    "     \n",
    "        #take all p in 'plot'(second section)\n",
    "        b=True\n",
    "        if soup.find('span', attrs={'class': 'mw-headline'}): \n",
    "            \n",
    "            heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "            if heading.find_all_next('p'):\n",
    "                \n",
    "                paragraphs = heading.find_all_next('p')\n",
    "                for p in paragraphs: \n",
    "                    # print (team.text)\n",
    "                    plot=plot+p.text\n",
    "                    if p.next_sibiling:\n",
    "                    \n",
    "                        if not p.next_sibling.name=='p':\n",
    "                            b=False\n",
    "                        if not b:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                plot=\"NAN\"\n",
    "        else:\n",
    "            plot=\"NAN\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        intro=\"NAN\"\n",
    "        plot=\"NAN\"\n",
    "    \n",
    "    #here: code to get info about infobox from every page    \n",
    "    if soup.find('table', attrs={'class': 'infobox vevent'}):\n",
    "        \n",
    "        table = soup.find('table', attrs={'class': 'infobox vevent'})  \n",
    "    \n",
    "        if table.find('th', attrs={'class': 'summary'}):\n",
    "        \n",
    "            x=table.find('th', attrs={'class': 'summary'})\n",
    "            title_name=x.text.strip()\n",
    "        \n",
    "        for cell in table.find_all('th'):\n",
    "        \n",
    "            if cell.find_next_sibling('td'):\n",
    "                a=cell.find_next_sibling('td')\n",
    "                if cell.text.strip()=='Directed by':\n",
    "                    director=a.text.strip()\n",
    "                elif cell.text.strip()=='Produced by':\n",
    "                \n",
    "                    producer=a.text.strip()\n",
    "                elif cell.text.strip()=='Written by':\n",
    "                \n",
    "                    writer=a.text.strip()\n",
    "                elif cell.text.strip()=='Starring':\n",
    "              \n",
    "                    starring=a.text.strip()\n",
    "                elif cell.text.strip()=='Music by':\n",
    "                \n",
    "                    music=a.text.strip()\n",
    "                elif cell.text.strip()=='Release date':\n",
    "                    release_date=a.text.strip()   \n",
    "                elif cell.text.strip()=='Running time':\n",
    "                \n",
    "                    runtime=a.text.strip()\n",
    "                elif cell.text.strip()=='Country':\n",
    "              \n",
    "                    country=a.text.strip()\n",
    "                elif cell.text.strip()=='Language':\n",
    "              \n",
    "                    language=a.text.strip()\n",
    "                elif cell.text.strip()=='Budget':\n",
    "              \n",
    "                    budget=a.text.strip()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    #put all infos in movie list\n",
    "    movie=[title,intro,plot,title_name,producer,director,writer,starring,music,release_date,runtime,country,language,budget]\n",
    "    #update dataframe with this list\n",
    "    extension2=\".tsv\"\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "   \n",
    "    movieTitle=[\"title\",\"intro\",\"plot\",\"title_name\",\"producer\",\"director\",\"writer\",\"starring\",\"music\",\"release_date\",\"runtime\",\"country\",\"language\",\"budget\"]\n",
    "    with open('tsv/'+file, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_output.writerow(movieTitle)\n",
    "        tsv_output.writerow(movie)\n",
    "    df.loc[index] = movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"aritcle_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(len(listUrl_Movies3)):\n",
    "\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"tsv/\"+file,\"r\") as tsvfile, open(\"tsv_correct/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(set(map(str.lower, row[i])))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "#create vocabulary and save it on vocabulary.tsv\n",
    "            \n",
    "\n",
    "dict1 = dict()\n",
    "term_id=0\n",
    "present=False\n",
    "with open('tsv/vocobulary.tsv', 'w', newline='') as f_output:\n",
    "        tsv_vocabulary = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_vocabulary.writerow(['word','term_id'])\n",
    "        name=\"aritcle_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for index in range(len(listUrl_Movies3)):\n",
    "            h+=1\n",
    "            print(h)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                #put in intro a list of all words that we have in intro of i-th page\n",
    "                intro=data_list[1][2]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                #put in plot a list of all words that we have in plot of i-th page\n",
    "                plot=data_list[1][1]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                \n",
    "                #put in text, a list that contains all words that are in plot and word for every page (no duplicate)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #put in dict1 every words with its term_id (no duplicate)\n",
    "                for i in text:\n",
    "                    if i in dict1:    \n",
    "                        continue\n",
    "                    else:\n",
    "                        dict1[i]=term_id\n",
    "                        term_id+=1\n",
    "                \n",
    "        #put dict1 element in vocabulary.tsv file                \n",
    "        for key, val in dict1.items():\n",
    "                    tsv_vocabulary.writerow([key, val])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "with open('tsv/vocobulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        name=\"aritcle_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for row in tsv_vocabulary:\n",
    "            \n",
    "            dict2[row[1]]=[]\n",
    "        for index in range(len(listUrl_Movies3)):\n",
    "            h+=1\n",
    "            print(h)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "                for i in text:\n",
    "                    for row in tsv_vocabulary:\n",
    "                        if i==row[0]:\n",
    "                            doc=\"document_\"\n",
    "                            name2=\"{}{}\".format(doc,index)\n",
    "                            \n",
    "                            dict2[row[1]].append(name2)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        #put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "        with open('tsv/index2.tsv', 'w', newline='') as f_output:\n",
    "            tsv_vocabulary = csv.writer(f_output, delimiter='\\t')           \n",
    "            for key, val in dict2.items():\n",
    "                tsv_vocabulary.writerow([key, val])           \n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "#define function that allows us to calculate a list that is an intersection from two list\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "def getDocuments(words):\n",
    "\n",
    "\n",
    "    #we use dict3 to store term_id and its respective documents_id\n",
    "    dict3={}\n",
    "    ##we use dict4 to store evry word and its respective documents_id\n",
    "    dict4={}\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    #in listWords we have a list that contains all words about inout query\n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "\n",
    "    #with vocabulary.tsv we start to build a dict3 with term_id for every words in wordsList\n",
    "    with open('HW3 ADM/tsv/vocobulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for word in listWords:\n",
    "            word=word.lower()\n",
    "            present=False\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    dict3[row[1]]=[]\n",
    "                    present=True\n",
    "            #case where word is not in vocabulary\n",
    "            if present==False:\n",
    "                dict4[word]=[]\n",
    "\n",
    "        #we continue to match dicumnets_id to every term_id in dict3\n",
    "        with open('HW3 ADM/tsv/index2.tsv', 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            for k in dict3.keys():  \n",
    "                for row in tsv_index:\n",
    "                    if row[0]==k:\n",
    "                        dict3[k]=row[1]\n",
    "                        continue\n",
    "\n",
    "\n",
    "        #finally we build dict4 where evry word matches to respective documents_id\n",
    "        for k in dict3.keys():\n",
    "\n",
    "            for row in tsv_vocabulary:\n",
    "                if k==row[1]:\n",
    "                    dict4[row[0]]=dict3[row[1]]\n",
    "\n",
    "        document=ast.literal_eval(dict4[listWords[0]])         \n",
    "        #interection between every list in values dict4. In this way we have documnets_id where all words (in query input) are present\n",
    "        for value in dict4.values():\n",
    "            document=intersection(document,ast.literal_eval(value))\n",
    "        #print(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def searchEngine1(words):\n",
    "        #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words)\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url'])\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        url=listUrl_Movies3[int(numberDocument)]\n",
    "        name=\"aritcle_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='enormous damage unless something is done immediately'\n",
    "searchEngine1(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH ENGINE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import string\n",
    "\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"aritcle_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(len(listUrl_Movies3)):\n",
    "\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"tsv/\"+file,\"r\") as tsvfile, open(\"tsv_correct2/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(map(str.lower, row[i]))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "documents=[]\n",
    "name='aritcle_'\n",
    "extension2='.tsv'\n",
    "with open('HW3 ADM/tsv/vocobulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for index in range(0,10000):\n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text=list(map(str.lower, text))\n",
    "                documents.append(' '.join(text))\n",
    "                #print(documents)\n",
    "                # text2= list(set(map(str.lower, text)))\n",
    "                \n",
    "                \n",
    "        vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "        vectors = vectorizer.fit_transform(documents)\n",
    "        #print(vectors)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        df2 = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "dict={}\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "\n",
    "\n",
    "print('step 2')\n",
    "name=\"aritcle_\"\n",
    "extension2=\".tsv\"\n",
    "h=0\n",
    "for row in tsv_vocabulary:\n",
    "    h+=1\n",
    "    dict[row[0]]=row[1]\n",
    "    print(h)\n",
    "    dict2[row[1]]=[]\n",
    "for index in range(0,10000):\n",
    "    print(\"numero documento \"+ str(index))\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "        data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        intro=data_list[1][1]\n",
    "        intro = ast.literal_eval(intro)\n",
    "        plot=data_list[1][2]\n",
    "        plot = ast.literal_eval(plot)\n",
    "        text=plot+intro\n",
    "        text=list(map(str.lower, text))\n",
    "    \n",
    "        text2= list(set(map(str.lower, text)))\n",
    "        #print(text2)\n",
    "                        #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "        for i in text2:\n",
    "\n",
    "                            #print(\"word   \"+str(i))\n",
    "            # print(\"word \"+str(i))\n",
    "            # print(\"aaa\" + str(df2.iloc[index][i]))\n",
    "            res=df2.iloc[index][i]\n",
    "                            #print(\"res   \"+str(res))\n",
    "            for term in dict:\n",
    "                if i==term:\n",
    "                    print(\"key \"+ str(term))\n",
    "                    print(\"value \"+ str(dict[term]))\n",
    "                    doc=\"document_\"\n",
    "                    name2=\"{}{}\".format(doc,index)\n",
    "                    result=[name2,res]\n",
    "                    dict2[dict[term]].append(result)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "\n",
    "\n",
    "with open('HW3 ADM/tsv/index.tsv', 'w', newline='') as f_output:\n",
    "    tsv_index2 = csv.writer(f_output, delimiter='\\t')           \n",
    "    for key, val in dict2.items():\n",
    "        tsv_index2.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def getTfidf_query(query):\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "    vectors = vectorizer.fit_transform([query])\n",
    "        #print(vectors)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df_query = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfidf_document(words,document_id):\n",
    "    dict={}\n",
    "    \n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    #print(listWords)\n",
    "    df=pd.DataFrame(columns=listWords)\n",
    "    tfIdf=[]\n",
    "    with open('HW3 ADM/tsv/vocobulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        with open('HW3 ADM/tsv/index.tsv', 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "\n",
    "        for word in listWords:\n",
    "            listDoc=[]\n",
    "            #print(\"word \" + word)\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    term=row[1]\n",
    "                    #print(\"teerm\" + str(term))\n",
    "                    break\n",
    "            for row in tsv_index:\n",
    "                if term==row[0]:\n",
    "\n",
    "                    listDoc=ast.literal_eval(row[1])\n",
    "\n",
    "                    #print(listDoc)\n",
    "                    break\n",
    "            for index in listDoc:\n",
    "\n",
    "                #print(index)\n",
    "                if index[0]==document_id:\n",
    "                    #print(index[0])\n",
    "\n",
    "                    tfIdf.append(index[1])\n",
    "                    #print(index[1])\n",
    "                    break\n",
    "\n",
    "        df.loc[0]=tfIdf\n",
    "        df=df.reindex(sorted(df.columns), axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def coisine(list_query,list_document):\n",
    "    \n",
    "    res=(cosine_similarity([list_query,list_document]))\n",
    "    #print(res)\n",
    "    return res[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def searchEngine2(words):\n",
    "    #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words)\n",
    "    df_query=getTfidf_query(words)\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url','similarity'])\n",
    "\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        df_document=getTfidf_document(words,document[index])\n",
    "        #print(df_document)\n",
    "        list_query=list(df_query.loc[0])\n",
    "        #print(list_query)\n",
    "        list_document=list(df_document.loc[0])\n",
    "        #print(list_document)\n",
    "        similiarity=coisine(list_document,list_query)\n",
    "        #print(\"simi \"+ str(similiarity))\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        url=listUrl_Movies3[int(numberDocument)]\n",
    "        name=\"aritcle_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url,similiarity]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "            df=df.sort_values(by=['similarity'],ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Under the Skin</td>\n",
       "      <td>\\nUnder the Skin is a 2013  science fiction fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Under_the_Skin_(...</td>\n",
       "      <td>0.986714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>Ezra</td>\n",
       "      <td>\\nEzra is a 2017 Indian Malayalam-language sup...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ezra_(2017_film)</td>\n",
       "      <td>0.951631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>The Legend of the 7 Golden Vampires</td>\n",
       "      <td>\\nThe Legend of the 7 Golden Vampires is a 197...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Legend_of_th...</td>\n",
       "      <td>0.926641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>The Legend of the 7 Golden Vampires</td>\n",
       "      <td>\\nThe Legend of the 7 Golden Vampires is a 197...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Legend_of_th...</td>\n",
       "      <td>0.926641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>On the Job</td>\n",
       "      <td>\\nOn the Job (abbreviated OTJ) is a 2013 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/On_the_Job_(2013...</td>\n",
       "      <td>0.889190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>Anna and the Apocalypse</td>\n",
       "      <td>\\nAnna and the Apocalypse is a 2017 British Ch...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Anna_and_the_Apo...</td>\n",
       "      <td>0.830832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>The Changeling</td>\n",
       "      <td>The Changeling is a 1980 Canadian supernatural...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Changeling_(...</td>\n",
       "      <td>0.825027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>Dilwale Dulhania Le Jayenge</td>\n",
       "      <td>\\nDilwale Dulhania Le Jayenge (transl. The Big...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dilwale_Dulhania...</td>\n",
       "      <td>0.803358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title  \\\n",
       "1136                       Under the Skin   \n",
       "8113                                 Ezra   \n",
       "63    The Legend of the 7 Golden Vampires   \n",
       "2687  The Legend of the 7 Golden Vampires   \n",
       "3440                           On the Job   \n",
       "1400              Anna and the Apocalypse   \n",
       "1439                       The Changeling   \n",
       "5380          Dilwale Dulhania Le Jayenge   \n",
       "\n",
       "                                                  intro  \\\n",
       "1136  \\nUnder the Skin is a 2013  science fiction fi...   \n",
       "8113  \\nEzra is a 2017 Indian Malayalam-language sup...   \n",
       "63    \\nThe Legend of the 7 Golden Vampires is a 197...   \n",
       "2687  \\nThe Legend of the 7 Golden Vampires is a 197...   \n",
       "3440  \\nOn the Job (abbreviated OTJ) is a 2013 Phili...   \n",
       "1400  \\nAnna and the Apocalypse is a 2017 British Ch...   \n",
       "1439  The Changeling is a 1980 Canadian supernatural...   \n",
       "5380  \\nDilwale Dulhania Le Jayenge (transl. The Big...   \n",
       "\n",
       "                                                    url  similarity  \n",
       "1136  https://en.wikipedia.org/wiki/Under_the_Skin_(...    0.986714  \n",
       "8113     https://en.wikipedia.org/wiki/Ezra_(2017_film)    0.951631  \n",
       "63    https://en.wikipedia.org/wiki/The_Legend_of_th...    0.926641  \n",
       "2687  https://en.wikipedia.org/wiki/The_Legend_of_th...    0.926641  \n",
       "3440  https://en.wikipedia.org/wiki/On_the_Job_(2013...    0.889190  \n",
       "1400  https://en.wikipedia.org/wiki/Anna_and_the_Apo...    0.830832  \n",
       "1439  https://en.wikipedia.org/wiki/The_Changeling_(...    0.825027  \n",
       "5380  https://en.wikipedia.org/wiki/Dilwale_Dulhania...    0.803358  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='life 2019 horror story'\n",
    "\n",
    "searchEngine2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>Ezra</td>\n",
       "      <td>\\nEzra is a 2017 Indian Malayalam-language sup...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ezra_(2017_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>On the Job</td>\n",
       "      <td>\\nOn the Job (abbreviated OTJ) is a 2013 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/On_the_Job_(2013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Under the Skin</td>\n",
       "      <td>\\nUnder the Skin is a 2013  science fiction fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Under_the_Skin_(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>The Legend of the 7 Golden Vampires</td>\n",
       "      <td>\\nThe Legend of the 7 Golden Vampires is a 197...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Legend_of_th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>The Legend of the 7 Golden Vampires</td>\n",
       "      <td>\\nThe Legend of the 7 Golden Vampires is a 197...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Legend_of_th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>Dilwale Dulhania Le Jayenge</td>\n",
       "      <td>\\nDilwale Dulhania Le Jayenge (transl. The Big...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dilwale_Dulhania...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>Anna and the Apocalypse</td>\n",
       "      <td>\\nAnna and the Apocalypse is a 2017 British Ch...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Anna_and_the_Apo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>The Changeling</td>\n",
       "      <td>The Changeling is a 1980 Canadian supernatural...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Changeling_(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title  \\\n",
       "8113                                 Ezra   \n",
       "3440                           On the Job   \n",
       "1136                       Under the Skin   \n",
       "63    The Legend of the 7 Golden Vampires   \n",
       "2687  The Legend of the 7 Golden Vampires   \n",
       "5380          Dilwale Dulhania Le Jayenge   \n",
       "1400              Anna and the Apocalypse   \n",
       "1439                       The Changeling   \n",
       "\n",
       "                                                  intro  \\\n",
       "8113  \\nEzra is a 2017 Indian Malayalam-language sup...   \n",
       "3440  \\nOn the Job (abbreviated OTJ) is a 2013 Phili...   \n",
       "1136  \\nUnder the Skin is a 2013  science fiction fi...   \n",
       "63    \\nThe Legend of the 7 Golden Vampires is a 197...   \n",
       "2687  \\nThe Legend of the 7 Golden Vampires is a 197...   \n",
       "5380  \\nDilwale Dulhania Le Jayenge (transl. The Big...   \n",
       "1400  \\nAnna and the Apocalypse is a 2017 British Ch...   \n",
       "1439  The Changeling is a 1980 Canadian supernatural...   \n",
       "\n",
       "                                                    url  \n",
       "8113     https://en.wikipedia.org/wiki/Ezra_(2017_film)  \n",
       "3440  https://en.wikipedia.org/wiki/On_the_Job_(2013...  \n",
       "1136  https://en.wikipedia.org/wiki/Under_the_Skin_(...  \n",
       "63    https://en.wikipedia.org/wiki/The_Legend_of_th...  \n",
       "2687  https://en.wikipedia.org/wiki/The_Legend_of_th...  \n",
       "5380  https://en.wikipedia.org/wiki/Dilwale_Dulhania...  \n",
       "1400  https://en.wikipedia.org/wiki/Anna_and_the_Apo...  \n",
       "1439  https://en.wikipedia.org/wiki/The_Changeling_(...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchEngine1(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
