{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "f = open(\"/Users/digitalfirst/Desktop/HW3 ADM/movies3.html\")\n",
    "\n",
    "soup = BeautifulSoup(f)\n",
    "listUrl_Movies3=[]\n",
    "for link in soup.select('a'):\n",
    "    listUrl_Movies3.append(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile(index):\n",
    "\n",
    "    for index in range(len(listUrl_Movies3)):\n",
    "        \n",
    "        t2=1200\n",
    "        try:\n",
    "            t1 = random.randint(1,5)\n",
    "            time.sleep(t1)\n",
    "            url=listUrl_Movies3[index]\n",
    "            response = requests.get(url)\n",
    "            name=\"aritcle_\"\n",
    "            extension=\".html\"\n",
    "            file=\"{}{}{}\".format(name,index,extension)\n",
    "            with open(file,'wb') as f: \n",
    "                f.write(response.content)  \n",
    "\n",
    "        except response.status_code as e:\n",
    "            print(\"exception\")\n",
    "            if e==492:\n",
    "                time.sleep(t2)\n",
    "                downloadFile(index+1)\n",
    "            elif e==200:\n",
    "                soup = BeautifulSoup(listUrl_Movies3[1])\n",
    "                \n",
    "                with open(file,'w') as f: \n",
    "                    f.write(soup.text)\n",
    "                downloadFile(index+1)\n",
    "            else:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadFile(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os.path\n",
    "#define column of our dataframe\n",
    "df=pd.DataFrame(columns=['title', 'intro', 'plot','film_name','producer','director','writer','starring','music','release date','runtime','country','language','budget'])\n",
    "\n",
    "\n",
    "for index in range(len(listUrl_Movies3)):\n",
    "    \n",
    "    title=''\n",
    "    plot=''\n",
    "    intro=''\n",
    "    title_name='NA'\n",
    "    producer='NA'\n",
    "    director='NA'\n",
    "    writer='NA'\n",
    "    starring='NA'\n",
    "    music='NA'\n",
    "    release_date='NA'\n",
    "    runtime='NA'\n",
    "    country='NA'\n",
    "    language='NA'\n",
    "    budget='NA'\n",
    "    \n",
    "    \n",
    "    #define name of the file that we want to find (in my case: in the same directory)\n",
    "    name=\"aritcle_\"\n",
    "    extension=\".html\"\n",
    "    file=\"{}{}{}\".format(name,index,extension)\n",
    "    \n",
    "    #check if this file exists\n",
    "    if not os.path.isfile(file):\n",
    "        continue\n",
    "        \n",
    "    #open file   \n",
    "    response2 = open(file)\n",
    "    soup = BeautifulSoup(response2)\n",
    "    #take title.\n",
    "    title=soup.title.text.rsplit(' ', 2)[0]\n",
    "    \n",
    "    #take all p in intro(firt section)\n",
    "    if soup.find('span', attrs={'class': 'mw-headline'}):\n",
    "        \n",
    "        heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "        paragraphs = heading.find_all_previous('p')\n",
    "        for p in paragraphs: \n",
    "            intro = p.text + intro\n",
    "            \n",
    "     \n",
    "        #take all p in 'plot'(second section)\n",
    "        b=True\n",
    "        if soup.find('span', attrs={'class': 'mw-headline'}): \n",
    "            \n",
    "            heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "            if heading.find_all_next('p'):\n",
    "                \n",
    "                paragraphs = heading.find_all_next('p')\n",
    "                for p in paragraphs: \n",
    "                    # print (team.text)\n",
    "                    plot=plot+p.text\n",
    "                    if p.next_sibiling:\n",
    "                    \n",
    "                        if not p.next_sibling.name=='p':\n",
    "                            b=False\n",
    "                        if not b:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                plot=\"NAN\"\n",
    "        else:\n",
    "            plot=\"NAN\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        intro=\"NAN\"\n",
    "        plot=\"NAN\"\n",
    "    \n",
    "    #here: code to get info about infobox from every page    \n",
    "    if soup.find('table', attrs={'class': 'infobox vevent'}):\n",
    "        \n",
    "        table = soup.find('table', attrs={'class': 'infobox vevent'})  \n",
    "    \n",
    "        if table.find('th', attrs={'class': 'summary'}):\n",
    "        \n",
    "            x=table.find('th', attrs={'class': 'summary'})\n",
    "            title_name=x.text.strip()\n",
    "        \n",
    "        for cell in table.find_all('th'):\n",
    "        \n",
    "            if cell.find_next_sibling('td'):\n",
    "                a=cell.find_next_sibling('td')\n",
    "                if cell.text.strip()=='Directed by':\n",
    "                    director=a.text.strip()\n",
    "                elif cell.text.strip()=='Produced by':\n",
    "                \n",
    "                    producer=a.text.strip()\n",
    "                elif cell.text.strip()=='Written by':\n",
    "                \n",
    "                    writer=a.text.strip()\n",
    "                elif cell.text.strip()=='Starring':\n",
    "              \n",
    "                    starring=a.text.strip()\n",
    "                elif cell.text.strip()=='Music by':\n",
    "                \n",
    "                    music=a.text.strip()\n",
    "                elif cell.text.strip()=='Release date':\n",
    "                    release_date=a.text.strip()   \n",
    "                elif cell.text.strip()=='Running time':\n",
    "                \n",
    "                    runtime=a.text.strip()\n",
    "                elif cell.text.strip()=='Country':\n",
    "              \n",
    "                    country=a.text.strip()\n",
    "                elif cell.text.strip()=='Language':\n",
    "              \n",
    "                    language=a.text.strip()\n",
    "                elif cell.text.strip()=='Budget':\n",
    "              \n",
    "                    budget=a.text.strip()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    #put all infos in movie list\n",
    "    movie=[title,intro,plot,title_name,producer,director,writer,starring,music,release_date,runtime,country,language,budget]\n",
    "    #update dataframe with this list\n",
    "    extension2=\".tsv\"\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "   \n",
    "    movieTitle=[\"title\",\"intro\",\"plot\",\"title_name\",\"producer\",\"director\",\"writer\",\"starring\",\"music\",\"release_date\",\"runtime\",\"country\",\"language\",\"budget\"]\n",
    "    with open('tsv/'+file, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_output.writerow(movieTitle)\n",
    "        tsv_output.writerow(movie)\n",
    "    df.loc[index] = movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"aritcle_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(len(listUrl_Movies3)):\n",
    "\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"tsv/\"+file,\"r\") as tsvfile, open(\"tsv_correct/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(set(map(str.lower, row[i])))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "#create vocabulary and save it on vocabulary.tsv\n",
    "            \n",
    "\n",
    "dict1 = dict()\n",
    "term_id=0\n",
    "present=False\n",
    "with open('tsv/vocobulary.tsv', 'w', newline='') as f_output:\n",
    "        tsv_vocabulary = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_vocabulary.writerow(['word','term_id'])\n",
    "        name=\"aritcle_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for index in range(len(listUrl_Movies3)):\n",
    "            h+=1\n",
    "            print(h)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                #put in intro a list of all words that we have in intro of i-th page\n",
    "                intro=data_list[1][2]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                #put in plot a list of all words that we have in plot of i-th page\n",
    "                plot=data_list[1][1]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                \n",
    "                #put in text, a list that contains all words that are in plot and word for every page (no duplicate)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #put in dict1 every words with its term_id (no duplicate)\n",
    "                for i in text:\n",
    "                    if i in dict1:    \n",
    "                        continue\n",
    "                    else:\n",
    "                        dict1[i]=term_id\n",
    "                        term_id+=1\n",
    "                \n",
    "        #put dict1 element in vocabulary.tsv file                \n",
    "        for key, val in dict1.items():\n",
    "                    tsv_vocabulary.writerow([key, val])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "with open('tsv/vocobulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        name=\"aritcle_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for row in tsv_vocabulary:\n",
    "            \n",
    "            dict2[row[1]]=[]\n",
    "        for index in range(len(listUrl_Movies3)):\n",
    "            h+=1\n",
    "            print(h)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "                for i in text:\n",
    "                    for row in tsv_vocabulary:\n",
    "                        if i==row[0]:\n",
    "                            doc=\"document_\"\n",
    "                            name2=\"{}{}\".format(doc,index)\n",
    "                            \n",
    "                            dict2[row[1]].append(name2)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        #put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "        with open('tsv/index2.tsv', 'w', newline='') as f_output:\n",
    "            tsv_vocabulary = csv.writer(f_output, delimiter='\\t')           \n",
    "            for key, val in dict2.items():\n",
    "                tsv_vocabulary.writerow([key, val])           \n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document_12']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "#define function that allows us to calculate a list that is an intersection from two list\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "#INPUT QUERY\n",
    "words='enormous damage unless something is done immediately'\n",
    "#we use dict3 to store term_id and its respective documents_id\n",
    "dict3={}\n",
    "##we use dict4 to store evry word and its respective documents_id\n",
    "dict4={}\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#in listWords we have a list that contains all words about inout query\n",
    "listWords = words.split()\n",
    "listWords=[x.lower() for x in listWords]\n",
    "\n",
    "#with vocabulary.tsv we start to build a dict3 with term_id for every words in wordsList\n",
    "with open('tsv/vocobulary.tsv', 'r', newline='') as f_output:\n",
    "    tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "    for word in listWords:\n",
    "        word=word.lower()\n",
    "        present=False\n",
    "        for row in tsv_vocabulary:\n",
    "            if word.lower()==row[0]:\n",
    "                dict3[row[1]]=[]\n",
    "                present=True\n",
    "        #case where word is not in vocabulary\n",
    "        if present==False:\n",
    "            dict4[word]=[]\n",
    "            \n",
    "    #we continue to match dicumnets_id to every term_id in dict3\n",
    "    with open('tsv/index2.tsv', 'r', newline='') as f_output:\n",
    "        tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for k in dict3.keys():  \n",
    "            for row in tsv_index:\n",
    "                if row[0]==k:\n",
    "                    dict3[k]=row[1]\n",
    "                    continue\n",
    "    \n",
    "    \n",
    "    #finally we build dict4 where evry word matches to respective documents_id\n",
    "    for k in dict3.keys():\n",
    "        \n",
    "        for row in tsv_vocabulary:\n",
    "            if k==row[1]:\n",
    "                dict4[row[0]]=dict3[row[1]]\n",
    "            \n",
    "    document=ast.literal_eval(dict4[listWords[0]])         \n",
    "    #interection between every list in values dict4. In this way we have documnets_id where all words (in query input) are present\n",
    "    for value in dict4.values():\n",
    "        document=intersection(document,ast.literal_eval(value))\n",
    "    print(document)\n",
    "          \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#build the dataframe with info for every documents_id\n",
    "df=pd.DataFrame(columns=['title', 'intro', 'url'])\n",
    "for index in range(len(document)):\n",
    "    #get id of documnets_is\n",
    "    numberDocument=document[index][9:]\n",
    "    #get wikipedia url\n",
    "    url=listUrl_Movies3[int(numberDocument)]\n",
    "    name=\"aritcle_\"\n",
    "    extension2=\".tsv\"\n",
    "    index=int(numberDocument)\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    #get info about title and intro for evert film that corresponds to every documents_id\n",
    "    with open('tsv/'+file, 'r', newline='') as f_output:\n",
    "        tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        title=tsv_index[1][3]\n",
    "        intro=tsv_index[1][1]\n",
    "        film=[title,intro,url]\n",
    "        #put all info for every film in a single row of df dataframe\n",
    "        df.loc[index] = film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Digby, the Biggest Dog in the World</td>\n",
       "      <td>\\nDigby, the Biggest Dog in the World is the t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Digby,_the_Bigge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title  \\\n",
       "12  Digby, the Biggest Dog in the World   \n",
       "\n",
       "                                                intro  \\\n",
       "12  \\nDigby, the Biggest Dog in the World is the t...   \n",
       "\n",
       "                                                  url  \n",
       "12  https://en.wikipedia.org/wiki/Digby,_the_Bigge...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
