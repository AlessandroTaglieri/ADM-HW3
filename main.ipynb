{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection:\n",
    "\n",
    "##  Get the list of movies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get a list of Wikipedia URLs from movies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "f = open(\"/Users/digitalfirst/Desktop/HW3 ADM/movies3.html\")\n",
    "\n",
    "soup = BeautifulSoup(f)\n",
    "listUrl_Movies3=[]\n",
    "for link in soup.select('a'):\n",
    "    listUrl_Movies3.append(link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Wikipedia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function 'downloadFile' that allow to download html file from URL list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile(index):\n",
    "\n",
    "    for index in range(len(listUrl_Movies3)):\n",
    "        #set time=20 mintes\n",
    "        t2=1200\n",
    "        try:\n",
    "            #wait 5 seconds bettween every request\n",
    "            t1 = random.randint(1,5)\n",
    "            time.sleep(t1)\n",
    "            url=listUrl_Movies3[index]\n",
    "            response = requests.get(url)\n",
    "            name=\"aritcle_\"\n",
    "            extension=\".html\"\n",
    "            file=\"{}{}{}\".format(name,index,extension)\n",
    "            with open(file,'wb') as f: \n",
    "                f.write(response.content)  \n",
    "\n",
    "        except response.status_code as e:\n",
    "            print(\"exception\")\n",
    "            #error=492 is error that occurs when we have done a limit of request\n",
    "            if e==492:\n",
    "                #wait 20 minutes \n",
    "                time.sleep(t2)\n",
    "                downloadFile(index+1)\n",
    "            elif e==200:\n",
    "                soup = BeautifulSoup(listUrl_Movies3[1])\n",
    "                \n",
    "                with open(file,'w') as f: \n",
    "                    f.write(soup.text)\n",
    "                downloadFile(index+1)\n",
    "            else:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadFile(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse downloaded pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create .tsv files for every html file and put them in tsv directory. These file contains data (title,intro,plot and infobox infos) as written in hw track. About starring in infobox, we saved every actor in a list (to do bonus section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os.path\n",
    "#define column of our dataframe\n",
    "df=pd.DataFrame(columns=['title', 'intro', 'plot','film_name','producer','director','writer','starring','music','release date','runtime','country','language','budget'])\n",
    "\n",
    "\n",
    "for index in range(20000,30000):\n",
    "    print(index)\n",
    "    title=''\n",
    "    plot=''\n",
    "    intro=''\n",
    "    title_name='NA'\n",
    "    producer='NA'\n",
    "    director='NA'\n",
    "    writer='NA'\n",
    "    starring=['NA']\n",
    "    music='NA'\n",
    "    release_date='NA'\n",
    "    runtime='NA'\n",
    "    country='NA'\n",
    "    language='NA'\n",
    "    budget='NA'\n",
    "    \n",
    "    \n",
    "    #define name of the file that we want to find (in my case: in the same directory)\n",
    "    name=\"article_\"\n",
    "    extension=\".html\"\n",
    "    file=\"{}{}{}\".format(name,index ,extension)\n",
    "    \n",
    "    #check if this file exists\n",
    "    if not os.path.isfile(\"HW3 ADM/\"+file):\n",
    "        continue\n",
    "        \n",
    "    #open file   \n",
    "    response2 = open(\"HW3 ADM/\"+file)\n",
    "    soup = BeautifulSoup(response2)\n",
    "    #take title.\n",
    "    title=soup.title.text.rsplit(' ', 2)[0]\n",
    "    \n",
    "    #take all p in intro(firt section)\n",
    "    #print(soup.find('span', attrs={'class': 'mw-headline'}))\n",
    "    if soup.find('span', attrs={'class': 'mw-headline'}):\n",
    "        heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "        paragraphs = heading.find_all_previous('p')\n",
    "        for p in paragraphs: \n",
    "            intro = p.text + intro\n",
    "            \n",
    "     \n",
    "        #take all p in 'plot'(second section)\n",
    "        b=True\n",
    "        #print(soup.find('span', attrs={'class': 'mw-headline'}))\n",
    "        if soup.find('span', attrs={'class': 'mw-headline'}): \n",
    "            \n",
    "            heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "            \n",
    "            for item in heading.parent.nextSiblingGenerator():\n",
    "                \n",
    "                if item.name=='h2':\n",
    "                    break\n",
    "                if hasattr(item, \"text\"):\n",
    "                    \n",
    "                    plot+=item.text\n",
    "\n",
    "        else:\n",
    "            plot=\"NAN\"\n",
    "    \n",
    "    else:\n",
    "        intro=\"NAN\"\n",
    "        plot=\"NAN\"\n",
    "    \n",
    "    \n",
    "    #Get info about infobox from every page and put them in respective sections in tsv file  \n",
    "    if soup.find('table', attrs={'class': 'infobox vevent'}):\n",
    "        \n",
    "        table = soup.find('table', attrs={'class': 'infobox vevent'})  \n",
    "    \n",
    "        if table.find('th', attrs={'class': 'summary'}):\n",
    "        \n",
    "            x=table.find('th', attrs={'class': 'summary'})\n",
    "            title_name=x.text.strip()\n",
    "        \n",
    "        for cell in table.find_all('th'):\n",
    "        \n",
    "            if cell.find_next_sibling('td'):\n",
    "                a=cell.find_next_sibling('td')\n",
    "                if cell.text.strip()=='Directed by':\n",
    "                    director=a.text.strip()\n",
    "                elif cell.text.strip()=='Produced by':\n",
    "                \n",
    "                    producer=a.text.strip()\n",
    "                elif cell.text.strip()=='Written by':\n",
    "                \n",
    "                    writer=a.text.strip()\n",
    "                elif cell.text.strip()=='Starring':\n",
    "                    listStarring=[]\n",
    "                    for link in a.select('a'):\n",
    "                        \n",
    "                        listStarring.append(link.text)\n",
    "                    starring=listStarring\n",
    "                    #print(starring)\n",
    "                elif cell.text.strip()=='Music by':\n",
    "                \n",
    "                    music=a.text.strip()\n",
    "                elif cell.text.strip()=='Release date':\n",
    "                    release_date=a.text.strip()   \n",
    "                elif cell.text.strip()=='Running time':\n",
    "                \n",
    "                    runtime=a.text.strip()\n",
    "                elif cell.text.strip()=='Country':\n",
    "              \n",
    "                    country=a.text.strip()\n",
    "                elif cell.text.strip()=='Language':\n",
    "              \n",
    "                    language=a.text.strip()\n",
    "                elif cell.text.strip()=='Budget':\n",
    "              \n",
    "                    budget=a.text.strip()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    \n",
    "    #put all infos in movie list\n",
    "    movie=[title,intro,plot,title_name,producer,director,writer,starring,music,release_date,runtime,country,language,budget]\n",
    "    #update dataframe with this list\n",
    "    extension2=\".tsv\"\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "   \n",
    "    movieTitle=[\"title\",\"intro\",\"plot\",\"title_name\",\"producer\",\"director\",\"writer\",\"starring\",\"music\",\"release_date\",\"runtime\",\"country\",\"language\",\"budget\"]\n",
    "    with open(\"HW3 ADM/tsv_new/\"+file, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_output.writerow(movieTitle)\n",
    "        tsv_output.writerow(movie)\n",
    "    df.loc[index] = movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine:\n",
    "\n",
    "## Search engine 1: Conjunctive query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create tsv files with preporcessed text. We have preprocessed all texts in tsv file (that we have just created) and we have put them in other tsv files tsv_correct directory. For every section we have a list of all word that are in respective section. We have also deleted duplicated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import csv\n",
    "\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(20000,30000):\n",
    "    print(index)\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv/\"+file,\"r\") as tsvfile, open(\"HW3 ADM/tsv_correct/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(set(map(str.lower, row[i])))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created a vocabulary that we stored in vocabulary.tsv file. Here we have all words thate we have in all tsv file (in intro and plot section as wirtten in hw track). Every word match with a unique term_id. This term_is is a simple counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "#create vocabulary and save it on vocabulary.tsv\n",
    "            \n",
    "\n",
    "dict1 = dict()\n",
    "term_id=0\n",
    "present=False\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'w', newline='') as f_output:\n",
    "        tsv_vocabulary = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_vocabulary.writerow(['word','term_id'])\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        \n",
    "        for index in range(20000,30000):\n",
    "           \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                #put in intro a list of all words that we have in intro of i-th page\n",
    "                intro=data_list[1][2]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                #put in plot a list of all words that we have in plot of i-th page\n",
    "                plot=data_list[1][1]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                \n",
    "                #put in text, a list that contains all words that are in plot and word for every page (no duplicate)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #put in dict1 every words with its term_id (no duplicate)\n",
    "                for i in text:\n",
    "                    if i in dict1:    \n",
    "                        continue\n",
    "                    else:\n",
    "                        dict1[i]=term_id\n",
    "                        term_id+=1\n",
    "                \n",
    "        #put dict1 element in vocabulary.tsv file                \n",
    "        for key, val in dict1.items():\n",
    "                    tsv_vocabulary.writerow([key, val])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we created the index from vocabulary.tsv and from all preprocessed tsv files.  \n",
    "For every words (in plot and intro) in all prorocessed tsv files, we found their term_id (from vocabulary) and for evey term_id we match a list of document where repsective word is present. \n",
    "* This index was used to search query words in every document about 1st and 2nd search engines. We saved it into index.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for row in tsv_vocabulary:\n",
    "            \n",
    "            dict2[row[1]]=[]\n",
    "        for index in range(20000,30000):\n",
    "            \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "                for i in text:\n",
    "                    for row in tsv_vocabulary:\n",
    "                        if i==row[0]:\n",
    "                            doc=\"document_\"\n",
    "                            name2=\"{}{}\".format(doc,index)\n",
    "                            \n",
    "                            dict2[row[1]].append(name2)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        #put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "        with open('HW3 ADM/tsv/index1.tsv', 'w', newline='') as f_output:\n",
    "            tsv_vocabulary = csv.writer(f_output, delimiter='\\t')           \n",
    "            for key, val in dict2.items():\n",
    "                tsv_vocabulary.writerow([key, val])           \n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we create getDocuments function that allow to find all documents where the input query is present.\n",
    "This fucntion has two parameters in input: words that is our query and index is number of indext that we consider to do specific search engine. \n",
    "This function return a list of documents that contain input query based on index.\n",
    "* We use index1 about search engine 1 and search engine 2; index2 about search engine 3.\n",
    "\n",
    "* This function is used from all three different search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "#define function that allows us to calculate a list that is an intersection from two list\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "def getDocuments(words,index):\n",
    "\n",
    "\n",
    "    #we use dict3 to store term_id and its respective documents_id\n",
    "    dict3={}\n",
    "    ##we use dict4 to store evry word and its respective documents_id\n",
    "    dict4={}\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    #in listWords we have a list that contains all words about inout query\n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    \n",
    "    #with vocabulary.tsv we start to build a dict3 with term_id for every words in wordsList\n",
    "    with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for word in listWords:\n",
    "            word=word.lower()\n",
    "            present=False\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    dict3[row[1]]=[]\n",
    "                    present=True\n",
    "            #case where word is not in vocabulary\n",
    "            if present==False:\n",
    "                dict4[word]=[]\n",
    "        indexFile=\"index\"+str(index)+\".tsv\"\n",
    "        #we continue to match documnets_id to every term_id in dict3\n",
    "        with open('HW3 ADM/tsv/'+indexFile, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            for k in dict3.keys():  \n",
    "                for row in tsv_index:\n",
    "                    if row[0]==k:\n",
    "                        dict3[k]=row[1]\n",
    "                        continue\n",
    "\n",
    "\n",
    "        #finally we build dict4 where evry word matches to respective documents_id\n",
    "        for k in dict3.keys():\n",
    "\n",
    "            for row in tsv_vocabulary:\n",
    "                if k==row[1]:\n",
    "                    dict4[row[0]]=dict3[row[1]]\n",
    "\n",
    "        document=ast.literal_eval(dict4[listWords[0]])         \n",
    "        #interection between every list in values dict4. In this way we have documnets_id where all words (in query input) are present\n",
    "        for value in dict4.values():\n",
    "            document=intersection(document,ast.literal_eval(value))\n",
    "        #print(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* searchEngine1 function allow to do the first search engine. It take an input query and returns a dataframe that contains the result. In this function we call getDocuments and for every document that contains input query we get some info from not preprocessed tsv file in tsv directory.\n",
    "\n",
    "* This result is a list of movies and for every movies we show only intro, title and URL link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def searchEngine1(words):\n",
    "        #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words,1)\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url'])\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        url=listUrl_Movies3[int(numberDocument)-20000]\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20012</th>\n",
       "      <td>Digby, the Biggest Dog in the World</td>\n",
       "      <td>\\nDigby, the Biggest Dog in the World is the t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Digby,_the_Bigge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title  \\\n",
       "20012  Digby, the Biggest Dog in the World   \n",
       "\n",
       "                                                   intro  \\\n",
       "20012  \\nDigby, the Biggest Dog in the World is the t...   \n",
       "\n",
       "                                                     url  \n",
       "20012  https://en.wikipedia.org/wiki/Digby,_the_Bigge...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='enormous damage unless something is done immediately'\n",
    "searchEngine1(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search engine 2: Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create tsv files from not preprocessed files. This is the same process that we've done for tsv file in tsv_correct dirctory. But in this case, we considerated also duplicate words. This is important to calculate TfIdf about second search engine.\n",
    "\n",
    "* These tsv file are stored in tsv_correct2 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import string\n",
    "import csv\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(20000,30000):\n",
    "    print(index)\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv/\"+file,\"r\") as tsvfile, open(\"HW3 ADM/tsv_correct2/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(map(str.lower, row[i]))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From these tsv file that we have just created, we create a dataframe 'df2'.\n",
    "This dataframe contains the TfIdf value for every match between word-document. Its columns are all different words that we have preproccesed tsv file (in intro and plot). Its rows are different document that are identified by document_id (id stays for the number of movies, example:document_222 stays for article_222.html/article_222.tsv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "documents=[]\n",
    "name='article_'\n",
    "extension2='.tsv'\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for index in range(20000,30000):\n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text=list(map(str.lower, text))\n",
    "                documents.append(' '.join(text))\n",
    "                #print(documents)\n",
    "                # text2= list(set(map(str.lower, text)))\n",
    "                \n",
    "                \n",
    "        vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "        vectors = vectorizer.fit_transform(documents)\n",
    "        #print(vectors)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        df2 = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created a new dictionary that has term_id as key (that we get from vocabulary by every words) and an array as value. This array contains a list of matching between document and respective TfIdf for respective word in this document. For example:\n",
    "key: \"122\", value:[[dcoument_12,0.02],[dcoument_18,0.22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "dict={}\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "\n",
    "\n",
    "#print('step 2')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "h=0\n",
    "for row in tsv_vocabulary:\n",
    "    h+=1\n",
    "    dict[row[0]]=row[1]\n",
    "    print(h)\n",
    "    dict2[row[1]]=[]\n",
    "for index in range(20000,30000):\n",
    "    #print(index)\n",
    "    print(\"numero documento \"+ str(index))\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "        data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        intro=data_list[1][1]\n",
    "        intro = ast.literal_eval(intro)\n",
    "        plot=data_list[1][2]\n",
    "        plot = ast.literal_eval(plot)\n",
    "        text=plot+intro\n",
    "        text=list(map(str.lower, text))\n",
    "    \n",
    "        text2= list(set(map(str.lower, text)))\n",
    "        #print(text2)\n",
    "                        #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "        for i in text2:\n",
    "\n",
    "                            #print(\"word   \"+str(i))\n",
    "            # print(\"word \"+str(i))\n",
    "            # print(\"aaa\" + str(df2.iloc[index][i]))\n",
    "            res=df2.iloc[index-20000][i]\n",
    "                            #print(\"res   \"+str(res))\n",
    "            for term in dict:\n",
    "                if i==term:\n",
    "                    #print(\"key \"+ str(term))\n",
    "                    #print(\"value \"+ str(dict[term]))\n",
    "                    doc=\"document_\"\n",
    "                    name2=\"{}{}\".format(doc,index)\n",
    "                    result=[name2,res]\n",
    "                    dict2[dict[term]].append(result)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We stored this dictionary in index2.tsv file. Then we have the index needed for search engine 2 to get TfIdf for a specific word in a specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "\n",
    "\n",
    "with open('HW3 ADM/tsv_new/index2.tsv', 'w', newline='') as f_output:\n",
    "    tsv_index2 = csv.writer(f_output, delimiter='\\t')           \n",
    "    for key, val in dict2.items():\n",
    "        tsv_index2.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This 'getTfidf_query' function allow to calculate TfIdf value about input query. It's needed because we must calculate coisine similiarity between result document and input query (based on TfIdf).\n",
    "It has input query as parameter and returns a dataframe that has tfIdf for every word in input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def getTfidf_query(query):\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "    vectors = vectorizer.fit_transform([query])\n",
    "        #print(vectors)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df_query = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This 'getTfidf_document' function allow to get TfIdf about every word in query for a specific document.\n",
    "This function searches this value from index2 (that we have just created with TfIdf values). It has as parameters: input query and document where we got TfIdf fro every word in query for this document.\n",
    "It returns a dataframe with these matchings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfidf_document(words,document_id):\n",
    "    dict={}\n",
    "    \n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    #print(listWords)\n",
    "    df=pd.DataFrame(columns=listWords)\n",
    "    tfIdf=[]\n",
    "    with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        with open('HW3 ADM/tsv/index2.tsv', 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            \n",
    "        for word in listWords:\n",
    "            listDoc=[]\n",
    "            #print(\"word \" + word)\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    term=row[1]\n",
    "                    #print(\"teerm\" + str(term))\n",
    "                    break\n",
    "            for row in tsv_index:\n",
    "                if term==row[0]:\n",
    "\n",
    "                    listDoc=ast.literal_eval(row[1])\n",
    "\n",
    "                    #print(listDoc)\n",
    "                    break\n",
    "            for index in listDoc:\n",
    "\n",
    "                #print(index)\n",
    "                if index[0]==document_id:\n",
    "                    #print(index[0])\n",
    "\n",
    "                    tfIdf.append(index[1])\n",
    "                    #print(index[1])\n",
    "                    break\n",
    "\n",
    "        df.loc[0]=tfIdf\n",
    "        df=df.reindex(sorted(df.columns), axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'coisine' function allow to calculate coisine similairity from two list of TfIdf values. First list is about TfIdf values for input query calcuated by 'getTfidf_query' function. Second list is about TfIdf values for query input in document, calculated by 'getTfidf_document' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def coisine(list_query,list_document):\n",
    "    \n",
    "    res=(cosine_similarity([list_query,list_document]))\n",
    "    #print(res)\n",
    "    return res[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'searchEngine2' function allow to calculate the result for the second search engine.\n",
    "We call 'getDocuments(words,1)' to get all documents that contain input query.\n",
    "Then we call 'getTfidf_query(words)' to put in 'df' datframe all TfIdf values froi every word in input query.\n",
    "Then, for every document, we call 'getTfidf_document' and put its result in 'df_document' dataframe.\n",
    "Finally we call 'coisine' fucntion to get coisine value for every input and put its result in a final 'df' dataframe with other info(we get these infos from tsv file (intro, title, and url)). \n",
    "It's ordered by this similiarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def searchEngine2(words):\n",
    "    #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words,1)\n",
    "    df_query=getTfidf_query(words)\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url','similarity'])\n",
    "\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        df_document=getTfidf_document(words,document[index])\n",
    "        \n",
    "        \n",
    "        #get Tfidf list from df_query dataframe\n",
    "        list_query=list(df_query.loc[0])\n",
    "        #get Tfidf list from df_document dataframe\n",
    "        list_document=list(df_document.loc[0])\n",
    "        \n",
    "        similiarity=coisine(list_document,list_query)\n",
    "        \n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        url=listUrl_Movies3[int(numberDocument)-20000]\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url,similiarity]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "            df=df.sort_values(by=['similarity'],ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22886</th>\n",
       "      <td>Paper Marriage</td>\n",
       "      <td>\\nPaper Marriage is a 1988 Hong Kong action co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Paper_Marriage</td>\n",
       "      <td>0.962736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21250</th>\n",
       "      <td>Candlestick</td>\n",
       "      <td>\\nCandlestick is a 2014 British film starring ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Candlestick_(film)</td>\n",
       "      <td>0.903011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20434</th>\n",
       "      <td>Four Weddings and a Funeral</td>\n",
       "      <td>\\nFour Weddings and a Funeral is a 1994 Britis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Four_Weddings_an...</td>\n",
       "      <td>0.698505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>Skyfall</td>\n",
       "      <td>\\nSkyfall is a 2012 British-American spy film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Skyfall</td>\n",
       "      <td>0.687316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23440</th>\n",
       "      <td>On the Job</td>\n",
       "      <td>\\nOn the Job (abbreviated OTJ) is a 2013 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/On_the_Job_(2013...</td>\n",
       "      <td>0.673693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26654</th>\n",
       "      <td>Simran</td>\n",
       "      <td>\\nSimran is a 2017 Indian heist crime drama fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Simran_(film)</td>\n",
       "      <td>0.666877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>Harry Potter and the Deathly Hallows – Part 2</td>\n",
       "      <td>\\nHarry Potter and the Deathly Hallows – Part ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Harry_Potter_and...</td>\n",
       "      <td>0.649515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25629</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>\\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Prince_of_Light</td>\n",
       "      <td>0.613451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "22886                                 Paper Marriage   \n",
       "21250                                    Candlestick   \n",
       "20434                    Four Weddings and a Funeral   \n",
       "21077                                        Skyfall   \n",
       "23440                                     On the Job   \n",
       "26654                                         Simran   \n",
       "20949  Harry Potter and the Deathly Hallows – Part 2   \n",
       "25629            Ramayana: The Legend of Prince Rama   \n",
       "\n",
       "                                                   intro  \\\n",
       "22886  \\nPaper Marriage is a 1988 Hong Kong action co...   \n",
       "21250  \\nCandlestick is a 2014 British film starring ...   \n",
       "20434  \\nFour Weddings and a Funeral is a 1994 Britis...   \n",
       "21077  \\nSkyfall is a 2012 British-American spy film ...   \n",
       "23440  \\nOn the Job (abbreviated OTJ) is a 2013 Phili...   \n",
       "26654  \\nSimran is a 2017 Indian heist crime drama fi...   \n",
       "20949  \\nHarry Potter and the Deathly Hallows – Part ...   \n",
       "25629  \\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...   \n",
       "\n",
       "                                                     url  similarity  \n",
       "22886       https://en.wikipedia.org/wiki/Paper_Marriage    0.962736  \n",
       "21250   https://en.wikipedia.org/wiki/Candlestick_(film)    0.903011  \n",
       "20434  https://en.wikipedia.org/wiki/Four_Weddings_an...    0.698505  \n",
       "21077              https://en.wikipedia.org/wiki/Skyfall    0.687316  \n",
       "23440  https://en.wikipedia.org/wiki/On_the_Job_(2013...    0.673693  \n",
       "26654        https://en.wikipedia.org/wiki/Simran_(film)    0.666877  \n",
       "20949  https://en.wikipedia.org/wiki/Harry_Potter_and...    0.649515  \n",
       "25629  https://en.wikipedia.org/wiki/The_Prince_of_Light    0.613451  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='In the United States 2019'\n",
    "\n",
    "searchEngine2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25629</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>\\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Prince_of_Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>Skyfall</td>\n",
       "      <td>\\nSkyfall is a 2012 British-American spy film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Skyfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23440</th>\n",
       "      <td>On the Job</td>\n",
       "      <td>\\nOn the Job (abbreviated OTJ) is a 2013 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/On_the_Job_(2013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22886</th>\n",
       "      <td>Paper Marriage</td>\n",
       "      <td>\\nPaper Marriage is a 1988 Hong Kong action co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Paper_Marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>Harry Potter and the Deathly Hallows – Part 2</td>\n",
       "      <td>\\nHarry Potter and the Deathly Hallows – Part ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Harry_Potter_and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21250</th>\n",
       "      <td>Candlestick</td>\n",
       "      <td>\\nCandlestick is a 2014 British film starring ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Candlestick_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20434</th>\n",
       "      <td>Four Weddings and a Funeral</td>\n",
       "      <td>\\nFour Weddings and a Funeral is a 1994 Britis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Four_Weddings_an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26654</th>\n",
       "      <td>Simran</td>\n",
       "      <td>\\nSimran is a 2017 Indian heist crime drama fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Simran_(film)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "25629            Ramayana: The Legend of Prince Rama   \n",
       "21077                                        Skyfall   \n",
       "23440                                     On the Job   \n",
       "22886                                 Paper Marriage   \n",
       "20949  Harry Potter and the Deathly Hallows – Part 2   \n",
       "21250                                    Candlestick   \n",
       "20434                    Four Weddings and a Funeral   \n",
       "26654                                         Simran   \n",
       "\n",
       "                                                   intro  \\\n",
       "25629  \\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...   \n",
       "21077  \\nSkyfall is a 2012 British-American spy film ...   \n",
       "23440  \\nOn the Job (abbreviated OTJ) is a 2013 Phili...   \n",
       "22886  \\nPaper Marriage is a 1988 Hong Kong action co...   \n",
       "20949  \\nHarry Potter and the Deathly Hallows – Part ...   \n",
       "21250  \\nCandlestick is a 2014 British film starring ...   \n",
       "20434  \\nFour Weddings and a Funeral is a 1994 Britis...   \n",
       "26654  \\nSimran is a 2017 Indian heist crime drama fi...   \n",
       "\n",
       "                                                     url  \n",
       "25629  https://en.wikipedia.org/wiki/The_Prince_of_Light  \n",
       "21077              https://en.wikipedia.org/wiki/Skyfall  \n",
       "23440  https://en.wikipedia.org/wiki/On_the_Job_(2013...  \n",
       "22886       https://en.wikipedia.org/wiki/Paper_Marriage  \n",
       "20949  https://en.wikipedia.org/wiki/Harry_Potter_and...  \n",
       "21250   https://en.wikipedia.org/wiki/Candlestick_(film)  \n",
       "20434  https://en.wikipedia.org/wiki/Four_Weddings_an...  \n",
       "26654        https://en.wikipedia.org/wiki/Simran_(film)  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchEngine1(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new score, Search Engine 3:\n",
    "###### We did 3rd search engine with \"zone index\" methods. We get info about this metohod from this link: https://moz.com/blog/search-engine-algorithm-basics.\n",
    "\n",
    "We conseidered these following sections for every movies: title, intro, plot and music. This method consist in to assigning a fixed score. In our case we have:\n",
    "* title: 0.9\n",
    "* intro: 0.4\n",
    "* plot: 0.3\n",
    "* music: 0.6\n",
    "\n",
    "Then, we search all documents that contain input query (in title/intro/plot/music).\n",
    "For every document we get its score that we calculate in the following way:\n",
    "This score is a sum of different score in every section. You can sum the score for every section if and only if:\n",
    "* score about titile: if query contains the whole title;\n",
    "* score about intro/plot: if intro/plot contains the whole query;\n",
    "* score about music: if at least one word of the query is in music section.\n",
    "\n",
    "With this scoring we should give more important to music and title section. \n",
    "We have choosen to give this score in the previous way about music because we want sum this score when only name or surname (or both) of music compositor is in the input query. About title, we want to give this score becuase title must be totally in the input query.\n",
    "About intro and plot, we have give these score because we think that intro contain words more significant than words in plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created index for the search engine 3 and we put it in index3.tsv file. In this index, we considered also 'music' section to match document for every word. It's similiar to index1.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for row in tsv_vocabulary:\n",
    "            \n",
    "            dict2[row[1]]=[]\n",
    "        for index in range(20000,30000):\n",
    "            \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                music=data_list[1][8]\n",
    "                music = ast.literal_eval(music)\n",
    "                text=plot+intro+music\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "                for i in text:\n",
    "                    for row in tsv_vocabulary:\n",
    "                        if i==row[0]:\n",
    "                            doc=\"document_\"\n",
    "                            name2=\"{}{}\".format(doc,index)\n",
    "                            \n",
    "                            dict2[row[1]].append(name2)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        #put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "        with open('HW3 ADM/tsv/index3.tsv', 'w', newline='') as f_output:\n",
    "            tsv_vocabulary = csv.writer(f_output, delimiter='\\t')           \n",
    "            for key, val in dict2.items():\n",
    "                tsv_vocabulary.writerow([key, val])           \n",
    "                    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following function 'heapSortK' allow us to use a heap data structure (using heapq library) for maintaining the top-k documents. This function has a list and a k value. This value is the number of ordered results that we wat to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def heapSortK( array, k ):\n",
    "    \n",
    "    heap = []\n",
    "    # Note: below is for illustration. It can be replaced by \n",
    "    # heapq.nlargest( bigArray, k )\n",
    "    #array=sorted(array, key = lambda x: x[4])\n",
    "    for item in array:\n",
    "        # If we have not yet found k items, or the current item is larger than\n",
    "        # the smallest item on the heap,\n",
    "        # print(\"item \"+str(item[4]))\n",
    "        if len(heap) < k or item[4] > heap[0][4]:\n",
    "            # If the heap is full, remove the smallest element on the heap.\n",
    "            heap=sorted(array, key = lambda x: x[4],reverse=True)\n",
    "            \n",
    "            if len(heap) == k: (heap.pop)\n",
    "            # add the current element as the new smallest.\n",
    "            heapq.heappush( heap, item )\n",
    "            #print(heap[0][4])\n",
    "    \n",
    "    heap=heap[:-1]\n",
    "    \n",
    "    return heap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 'search3' we have calculated the result for the 3rd search engine.\n",
    "Initially we have created a 'df_score' dataframe to store fixed score for every section (taht we considered). Then we call 'getDocuments(query,3)' to get all dcouments where all words in query are present on them (based on index3.tsv).\n",
    "\n",
    "For every document, we check the following aspects:\n",
    "* if all query words are in title section, sum score about title in 'df_score' to 'score' (final score);\n",
    "* if all query words are in intro section, sum score about intro in 'df_score' to 'score' (final score);\n",
    "* if all 'title_section' words are in input query, sum score about title in 'df_score' to 'score' (final score);\n",
    "* if at least on word in inout query is in 'music' section, sum score about music in 'df_score' to 'score' (final score);\n",
    "\n",
    "Final dataframe 'ndf' contains all movies with these following data (title,intro,plot,music, score). It's ordered by this score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "def search3(query,k):\n",
    "    \n",
    "    document=getDocuments(query,3)\n",
    "    listWords = query.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'plot', 'music', 'score'])\n",
    "    df_score=pd.DataFrame(columns=['title_score', 'intro_score', 'plot_score', 'music_score'])\n",
    "    scores=[0.8,0.4,0.3,0.6]\n",
    "    df_score.loc[0]=scores\n",
    "    resultMovies=[]\n",
    "    actors=[]\n",
    "    for index in range(len(document)):\n",
    "        score=0\n",
    "        lista=[]\n",
    "        #get id of documnets_is\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        url=listUrl_Movies3[int(numberDocument)-20000]\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                    tsv_index = list(csv.reader(tsvfile, delimiter='\\t'))\n",
    "                    title=ast.literal_eval(tsv_index[1][3])\n",
    "                    \n",
    "                    intro=ast.literal_eval(tsv_index[1][1])\n",
    "\n",
    "                    plot=ast.literal_eval(tsv_index[1][2])\n",
    "                    \n",
    "                    music=ast.literal_eval(tsv_index[1][8])\n",
    "                    #actors.append(tsv_index[1][7])\n",
    "                    \n",
    "                    if (all(elem in title  for elem in listWords)) or (all(elem in listWords  for elem in title)):\n",
    "                            score+=df_score.loc[0]['title_score']\n",
    "                            \n",
    "                    if all(elem in intro  for elem in listWords)==True:\n",
    "                            score+=df_score.loc[0]['intro_score']\n",
    "                            \n",
    "                    if all(elem in plot  for elem in listWords)==True:\n",
    "                            score+=df_score.loc[0]['plot_score']\n",
    "                            \n",
    "                    if any(elem in music  for elem in listWords)==True:\n",
    "                        \n",
    "                        score+=df_score.loc[0]['music_score']\n",
    "                    \n",
    "\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_file = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title2=tsv_file[1][3]\n",
    "            intro2=tsv_file[1][1]\n",
    "            plot2=tsv_file[1][2]\n",
    "            music2=tsv_file[1][8]\n",
    "            lista=ast.literal_eval(tsv_file[1][7])\n",
    "            actors.append(lista)\n",
    "            film=[title2,intro2,plot2,music2,score]\n",
    "            resultMovies.append(film)\n",
    "            \n",
    "            \n",
    "            #df.loc[index] = film\n",
    "           \n",
    "    list2=heapSortK(resultMovies,k)\n",
    "    j=0\n",
    "   \n",
    "    \n",
    "    for i in range(k):\n",
    "        if len(list2)>i:\n",
    "            df.loc[j] = list2[i]\n",
    "            j+=1\n",
    "        else:\n",
    "            break\n",
    "    #print(actors)\n",
    "    \n",
    "    graph(actors)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Step: Make a nice visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "def graph(lista):\n",
    "    G = nx.Graph() \n",
    "    couple=[]\n",
    "    count=0\n",
    "    for i in range(len(lista)):\n",
    "        for j in range(len(lista[i])):\n",
    "            for y in range(1,len(lista[i])):\n",
    "                couple=[lista[i][j],lista[i][y]]\n",
    "                count=0\n",
    "                for k in lista:\n",
    "                    if (couple[0] in k) and (couple[1] in k):\n",
    "                        \n",
    "                        count+=1\n",
    "                        if count>=2 and not couple[0] == couple[1]:\n",
    "                            \n",
    "                            G.add_edge(couple[0],couple[1])\n",
    "                            break\n",
    "     \n",
    " \n",
    "    pos = nx.spring_layout(G)   #<<<<<<<<<< Initialize this only once\n",
    "    nx.draw(G,pos=pos, with_labels=True, node_size = 100, font_size=10)  #<<<<<<<<< pass the pos variable\n",
    "    plt.draw() \n",
    "    plt.figure(figsize=(10,10)) # To plot the next graph in a new figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>plot</th>\n",
       "      <th>music</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grimsby</td>\n",
       "      <td>\\nGrimsby (released in the United States as Th...</td>\n",
       "      <td>\"Nobby\" Butcher has been separated from his li...</td>\n",
       "      <td>Erran Baron Cohen\\nDavid Buckley</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grimsby</td>\n",
       "      <td>\\nGrimsby (released in the United States as Th...</td>\n",
       "      <td>\"Nobby\" Butcher has been separated from his li...</td>\n",
       "      <td>Erran Baron Cohen\\nDavid Buckley</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before I Go to Sleep</td>\n",
       "      <td>\\nBefore I Go to Sleep is a 2014 mystery psych...</td>\n",
       "      <td>Forty-year-old Christine Lucas wakes up in bed...</td>\n",
       "      <td>Edward Shearmur</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Imitation Game</td>\n",
       "      <td>\\nThe Imitation Game is a 2014 American histor...</td>\n",
       "      <td>In 1951, two policemen, Nock and Staehl, inves...</td>\n",
       "      <td>Alexandre Desplat</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title                                              intro  \\\n",
       "0               Grimsby  \\nGrimsby (released in the United States as Th...   \n",
       "1               Grimsby  \\nGrimsby (released in the United States as Th...   \n",
       "2  Before I Go to Sleep  \\nBefore I Go to Sleep is a 2014 mystery psych...   \n",
       "3    The Imitation Game  \\nThe Imitation Game is a 2014 American histor...   \n",
       "\n",
       "                                                plot  \\\n",
       "0  \"Nobby\" Butcher has been separated from his li...   \n",
       "1  \"Nobby\" Butcher has been separated from his li...   \n",
       "2  Forty-year-old Christine Lucas wakes up in bed...   \n",
       "3  In 1951, two policemen, Nock and Staehl, inves...   \n",
       "\n",
       "                              music  score  \n",
       "0  Erran Baron Cohen\\nDavid Buckley    0.4  \n",
       "1  Erran Baron Cohen\\nDavid Buckley    0.4  \n",
       "2                   Edward Shearmur    0.4  \n",
       "3                 Alexandre Desplat    0.4  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='United States Strong Mark'\n",
    "search3(query,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "def graph(lista):\n",
    "    G = nx.Graph() \n",
    "    couple=[]\n",
    "    count=0\n",
    "    for i in range(len(lista)):\n",
    "        for j in range(len(lista[i])):\n",
    "            for y in range(1,len(lista[i])):\n",
    "                couple=[lista[i][j],lista[i][y]]\n",
    "                count=0\n",
    "                for k in lista:\n",
    "                    if (couple[0] in k) and (couple[1] in k):\n",
    "                        \n",
    "                        count+=1\n",
    "                        if count>=2 and not couple[0] == couple[1]:\n",
    "                            \n",
    "                            G.add_edge(couple[0],couple[1])\n",
    "                            break\n",
    "     \n",
    " \n",
    "    pos = nx.spring_layout(G)   #<<<<<<<<<< Initialize this only once\n",
    "    nx.draw(G,pos=pos, with_labels=True, node_size = 100, font_size=10)  #<<<<<<<<< pass the pos variable\n",
    "    plt.draw() \n",
    "    plt.figure(figsize=(10,10)) # To plot the next graph in a new figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
