{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection:\n",
    "\n",
    "##  Get the list of movies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get a list of Wikipedia URLs from movies1.html/ movies2.html/ movies3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "f1 = open(\"HW3 ADM/movies3.html\")\n",
    "f3 = open(\"HW3 ADM/movies1.html\")\n",
    "f2 = open(\"HW3 ADM/movies2.html\")\n",
    "soup = BeautifulSoup(f1)\n",
    "soup1 = BeautifulSoup(f3)\n",
    "soup2 = BeautifulSoup(f2)\n",
    "listUrl_Movies1=[]\n",
    "listUrl_Movies2=[]\n",
    "listUrl_Movies3=[]\n",
    "for link in soup.select('a'):\n",
    "    listUrl_Movies3.append(link.text)\n",
    "for link in soup2.select('a'):\n",
    "    listUrl_Movies2.append(link.text)\n",
    "for link in soup1.select('a'):\n",
    "    listUrl_Movies1.append(link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge these 3 lists in a single list that contains all 30 000 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalMovies=listUrl_Movies1+listUrl_Movies2+listUrl_Movies3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Wikipedia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function 'downloadFile' that allows to download html files from URL list\n",
    "\n",
    "We downloaded all movies. 10 000 per person. Every person downloaded file from:\n",
    "* listUrl_Movies1/ listUrl_Movies2/ listUrl_Movies3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile():\n",
    "\n",
    "    for index in range(listUrl_Movies1):\n",
    "        #set time=20 mintes\n",
    "        t2=1200\n",
    "        try:\n",
    "            #wait 5 seconds bettween every request\n",
    "            t1 = random.randint(1,5)\n",
    "            time.sleep(t1)\n",
    "            url=listUrl_Movies3[index]\n",
    "            response = requests.get(url)\n",
    "            name=\"article_\"\n",
    "            extension=\".html\"\n",
    "            file=\"{}{}{}\".format(name,index,extension)\n",
    "            with open(file,'wb') as f: \n",
    "                f.write(response.content)  \n",
    "\n",
    "        except response.status_code as e:\n",
    "            print(\"exception\")\n",
    "            #error=492 is error that occurs when we have done a limit of request\n",
    "            if e==492:\n",
    "                #wait 20 minutes \n",
    "                time.sleep(t2)\n",
    "                downloadFile(index+1)\n",
    "            elif e==200:\n",
    "                soup = BeautifulSoup(listUrl_Movies3[1])\n",
    "                \n",
    "                with open(file,'w') as f: \n",
    "                    f.write(soup.text)\n",
    "                downloadFile(index+1)\n",
    "            else:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse downloaded pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create .tsv files for every html file and put them in tsv directory. These files contain data (title,intro,plot and infobox infos) as written in hw track. About starring in infobox, we saved every actor in a list (to do bonus section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os.path\n",
    "#define column of our dataframe\n",
    "df=pd.DataFrame(columns=['title', 'intro', 'plot','film_name','producer','director','writer','starring','music','release date','runtime','country','language','budget'])\n",
    "\n",
    "\n",
    "for index in range(len(totalMovies)):\n",
    "    print(index)\n",
    "    title=''\n",
    "    plot=''\n",
    "    intro=''\n",
    "    title_name='NA'\n",
    "    producer='NA'\n",
    "    director='NA'\n",
    "    writer='NA'\n",
    "    starring=['NA']\n",
    "    music='NA'\n",
    "    release_date='NA'\n",
    "    runtime='NA'\n",
    "    country='NA'\n",
    "    language='NA'\n",
    "    budget='NA'\n",
    "    \n",
    "    \n",
    "    #define name of the file that we want to find (in my case: in the same directory)\n",
    "    name=\"article_\"\n",
    "    extension=\".html\"\n",
    "    file=\"{}{}{}\".format(name,index ,extension)\n",
    "    \n",
    "    #check if this file exists\n",
    "    if not os.path.isfile(\"HW3 ADM/\"+file):\n",
    "        continue\n",
    "        \n",
    "    #open file   \n",
    "    response2 = open(\"HW3 ADM/\"+file)\n",
    "    soup = BeautifulSoup(response2)\n",
    "    #take title.\n",
    "    title=soup.title.text.rsplit(' ', 2)[0]\n",
    "    \n",
    "    #take all p in intro(firt section)\n",
    "    #print(soup.find('span', attrs={'class': 'mw-headline'}))\n",
    "    if soup.find('span', attrs={'class': 'mw-headline'}):\n",
    "        heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "        paragraphs = heading.find_all_previous('p')\n",
    "        for p in paragraphs: \n",
    "            intro = p.text + intro\n",
    "            \n",
    "     \n",
    "        #take all p in 'plot'(second section)\n",
    "        b=True\n",
    "        #print(soup.find('span', attrs={'class': 'mw-headline'}))\n",
    "        if soup.find('span', attrs={'class': 'mw-headline'}): \n",
    "            \n",
    "            heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "            \n",
    "            for item in heading.parent.nextSiblingGenerator():\n",
    "                \n",
    "                if item.name=='h2':\n",
    "                    break\n",
    "                if hasattr(item, \"text\"):\n",
    "                    \n",
    "                    plot+=item.text\n",
    "\n",
    "        else:\n",
    "            plot=\"NAN\"\n",
    "    \n",
    "    else:\n",
    "        intro=\"NAN\"\n",
    "        plot=\"NAN\"\n",
    "    \n",
    "    \n",
    "    #Get info about infobox from every page and put them in respective sections in tsv file  \n",
    "    if soup.find('table', attrs={'class': 'infobox vevent'}):\n",
    "        \n",
    "        table = soup.find('table', attrs={'class': 'infobox vevent'})  \n",
    "    \n",
    "        if table.find('th', attrs={'class': 'summary'}):\n",
    "        \n",
    "            x=table.find('th', attrs={'class': 'summary'})\n",
    "            title_name=x.text.strip()\n",
    "        \n",
    "        for cell in table.find_all('th'):\n",
    "        \n",
    "            if cell.find_next_sibling('td'):\n",
    "                a=cell.find_next_sibling('td')\n",
    "                if cell.text.strip()=='Directed by':\n",
    "                    director=a.text.strip()\n",
    "                elif cell.text.strip()=='Produced by':\n",
    "                \n",
    "                    producer=a.text.strip()\n",
    "                elif cell.text.strip()=='Written by':\n",
    "                \n",
    "                    writer=a.text.strip()\n",
    "                elif cell.text.strip()=='Starring':\n",
    "                    listStarring=[]\n",
    "                    for link in a.select('a'):\n",
    "                        \n",
    "                        listStarring.append(link.text)\n",
    "                    starring=listStarring\n",
    "                    #print(starring)\n",
    "                elif cell.text.strip()=='Music by':\n",
    "                \n",
    "                    music=a.text.strip()\n",
    "                elif cell.text.strip()=='Release date':\n",
    "                    release_date=a.text.strip()   \n",
    "                elif cell.text.strip()=='Running time':\n",
    "                \n",
    "                    runtime=a.text.strip()\n",
    "                elif cell.text.strip()=='Country':\n",
    "              \n",
    "                    country=a.text.strip()\n",
    "                elif cell.text.strip()=='Language':\n",
    "              \n",
    "                    language=a.text.strip()\n",
    "                elif cell.text.strip()=='Budget':\n",
    "              \n",
    "                    budget=a.text.strip()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    \n",
    "    #put all infos in movie list\n",
    "    movie=[title,intro,plot,title_name,producer,director,writer,starring,music,release_date,runtime,country,language,budget]\n",
    "    #update dataframe with this list\n",
    "    extension2=\".tsv\"\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "   \n",
    "    movieTitle=[\"title\",\"intro\",\"plot\",\"title_name\",\"producer\",\"director\",\"writer\",\"starring\",\"music\",\"release_date\",\"runtime\",\"country\",\"language\",\"budget\"]\n",
    "    with open(\"HW3 ADM/tsv_new/\"+file, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_output.writerow(movieTitle)\n",
    "        tsv_output.writerow(movie)\n",
    "    df.loc[index] = movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine:\n",
    "\n",
    "## Search engine 1: Conjunctive query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create tsv files with preporcessed text. We have preprocessed all texts in tsv file (that we have just created) and we have put them in other tsv files in tsv_correct directory. For every section we have a list of all words that are in respective section. We have also deleted duplicated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import csv\n",
    "\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(len(totalMovies)):\n",
    "    print(index)\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv/\"+file,\"r\") as tsvfile, open(\"HW3 ADM/tsv_correct/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(set(map(str.lower, row[i])))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created a vocabulary that we stored in vocabulary.tsv file. Here we have all words that we have in all tsv files (in intro and plot section as wirtten in hw track). Every word matches with an unique term_id. This term_is is a simple counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "#create vocabulary and save it on vocabulary.tsv\n",
    "            \n",
    "\n",
    "dict1 = dict()\n",
    "term_id=0\n",
    "present=False\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'w', newline='') as f_output:\n",
    "        tsv_vocabulary = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_vocabulary.writerow(['word','term_id'])\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        \n",
    "        for index in range(len(totalMovies)):\n",
    "           \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                #put in intro a list of all words that we have in intro of i-th page\n",
    "                intro=data_list[1][2]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                #put in plot a list of all words that we have in plot of i-th page\n",
    "                plot=data_list[1][1]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                \n",
    "                #put in text, a list that contains all words that are in plot and word for every page (no duplicate)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #put in dict1 every words with its term_id (no duplicate)\n",
    "                for i in text:\n",
    "                    if i in dict1:    \n",
    "                        continue\n",
    "                    else:\n",
    "                        dict1[i]=term_id\n",
    "                        term_id+=1\n",
    "                \n",
    "        #put dict1 element in vocabulary.tsv file                \n",
    "        for key, val in dict1.items():\n",
    "                    tsv_vocabulary.writerow([key, val])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we created the index from vocabulary.tsv and from all preprocessed tsv files.  \n",
    "For every word (in plot and intro) in all prorocessed tsv files, we found their term_id (from vocabulary) and for evey term_id we match a list of document where repsective word is present. \n",
    "* This index was used to search query words in every document about 1st and 2nd search engines. We saved it into index.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for row in tsv_vocabulary:\n",
    "            \n",
    "            dict2[row[1]]=[]\n",
    "        for index in range(len(totalMovies)):\n",
    "            \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "                for i in text:\n",
    "                    for row in tsv_vocabulary:\n",
    "                        if i==row[0]:\n",
    "                            doc=\"document_\"\n",
    "                            name2=\"{}{}\".format(doc,index)\n",
    "                            \n",
    "                            dict2[row[1]].append(name2)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        #put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "        with open('HW3 ADM/tsv/index1.tsv', 'w', newline='') as f_output:\n",
    "            tsv_vocabulary = csv.writer(f_output, delimiter='\\t')           \n",
    "            for key, val in dict2.items():\n",
    "                tsv_vocabulary.writerow([key, val])           \n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we create getDocuments function that allows to find all documents where the input query is present.\n",
    "This fucntion has two parameters in input: words that is our query and index is the number of index that we consider to do specific search engine. \n",
    "This function returns a list of documents that contains input query based on index.\n",
    "* We use index1 about search engine 1 and search engine 2; index2 about search engine 3.\n",
    "\n",
    "* This function is used for all three different search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "#define function that allows us to calculate a list that is an intersection from two list\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "def getDocuments(words,index):\n",
    "\n",
    "\n",
    "    #we use dict3 to store term_id and its respective documents_id\n",
    "    dict3={}\n",
    "    ##we use dict4 to store evry word and its respective documents_id\n",
    "    dict4={}\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    #in listWords we have a list that contains all words about inout query\n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    \n",
    "    #with vocabulary.tsv we start to build a dict3 with term_id for every words in wordsList\n",
    "    with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for word in listWords:\n",
    "            word=word.lower()\n",
    "            present=False\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    dict3[row[1]]=[]\n",
    "                    present=True\n",
    "            #case where word is not in vocabulary\n",
    "            if present==False:\n",
    "                dict4[word]=[]\n",
    "        indexFile=\"index\"+str(index)+\".tsv\"\n",
    "        #we continue to match documnets_id to every term_id in dict3\n",
    "        with open('HW3 ADM/tsv/'+indexFile, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            for k in dict3.keys():  \n",
    "                for row in tsv_index:\n",
    "                    if row[0]==k:\n",
    "                        dict3[k]=row[1]\n",
    "                        continue\n",
    "\n",
    "\n",
    "        #finally we build dict4 where evry word matches to respective documents_id\n",
    "        for k in dict3.keys():\n",
    "\n",
    "            for row in tsv_vocabulary:\n",
    "                if k==row[1]:\n",
    "                    dict4[row[0]]=dict3[row[1]]\n",
    "\n",
    "        document=ast.literal_eval(dict4[listWords[0]])         \n",
    "        #return \"no results\" if any query words isn't at least in one document\n",
    "        for i in dict4.values():\n",
    "            if not i:\n",
    "                error='No results'\n",
    "                return error\n",
    "        #interection between every list in values dict4. In this way we have documnets_id where all words (in query input) are present\n",
    "        for value in dict4.values():\n",
    "            document=intersection(document,ast.literal_eval(value))\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* searchEngine1 function allows to do the first search engine. It takes an input query and returns a dataframe that contains the result. In this function we call getDocuments and for every document that contains input query we get some info from not preprocessed tsv file in tsv directory.\n",
    "\n",
    "* This result is a list of movies and for every movies we show only intro, title and URL link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def searchEngine1(words):\n",
    "        #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words,1)\n",
    "    if document=='No results':\n",
    "        return document\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url'])\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        \n",
    "        url=toalMovies[int(numberDocument)]\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20012</th>\n",
       "      <td>Digby, the Biggest Dog in the World</td>\n",
       "      <td>\\nDigby, the Biggest Dog in the World is the t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Digby,_the_Bigge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title  \\\n",
       "20012  Digby, the Biggest Dog in the World   \n",
       "\n",
       "                                                   intro  \\\n",
       "20012  \\nDigby, the Biggest Dog in the World is the t...   \n",
       "\n",
       "                                                     url  \n",
       "20012  https://en.wikipedia.org/wiki/Digby,_the_Bigge...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='enormous damage unless something is done immediately'\n",
    "searchEngine1(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search engine 2: Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create tsv files from not preprocessed files. This is the same process that we've done for tsv file in tsv_correct dirctory. But in this case, we considerated also duplicate words. This it is important to calculate TfIdf about second search engine.\n",
    "\n",
    "* These tsv file are stored in tsv_correct2 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import string\n",
    "import csv\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(len(totalMovies)):\n",
    "    print(index)\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv/\"+file,\"r\") as tsvfile, open(\"HW3 ADM/tsv_correct2/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(map(str.lower, row[i]))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From these tsv file that we have just created, we create a dataframe 'df2'.\n",
    "This dataframe contains the TfIdf value for every match between word-document. Its columns are all different words that we have preproccesed tsv file (in intro and plot). Its rows are different document that are identified by document_id (id stays for the number of movies, example:document_222 stays for article_222.html/article_222.tsv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "documents=[]\n",
    "name='article_'\n",
    "extension2='.tsv'\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for index in range(len(totalMovies)):\n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text=list(map(str.lower, text))\n",
    "                documents.append(' '.join(text))\n",
    "                #print(documents)\n",
    "                # text2= list(set(map(str.lower, text)))\n",
    "                \n",
    "                \n",
    "        vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "        vectors = vectorizer.fit_transform(documents)\n",
    "        #print(vectors)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        df2 = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created a new dictionary that has term_id as key (that we get from vocabulary by every word) and an array as value. This array contains a list of matching between document and respective TfIdf for respective word in this document. For example:\n",
    "key: \"122\", value:[[dcoument_12,0.02],[dcoument_18,0.22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "dict={}\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "\n",
    "\n",
    "#print('step 2')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "h=0\n",
    "for row in tsv_vocabulary:\n",
    "    h+=1\n",
    "    dict[row[0]]=row[1]\n",
    "    print(h)\n",
    "    dict2[row[1]]=[]\n",
    "for index in range(len(totalMovies)):\n",
    "    #print(index)\n",
    "    print(\"numero documento \"+ str(index))\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "        data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        intro=data_list[1][1]\n",
    "        intro = ast.literal_eval(intro)\n",
    "        plot=data_list[1][2]\n",
    "        plot = ast.literal_eval(plot)\n",
    "        text=plot+intro\n",
    "        text=list(map(str.lower, text))\n",
    "    \n",
    "        text2= list(set(map(str.lower, text)))\n",
    "        #print(text2)\n",
    "                        #for evry word in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "        for i in text2:\n",
    "\n",
    "                            #print(\"word   \"+str(i))\n",
    "            # print(\"word \"+str(i))\n",
    "            # print(\"aaa\" + str(df2.iloc[index][i]))\n",
    "            res=df2.iloc[index][i]\n",
    "                            #print(\"res   \"+str(res))\n",
    "            for term in dict:\n",
    "                if i==term:\n",
    "                    #print(\"key \"+ str(term))\n",
    "                    #print(\"value \"+ str(dict[term]))\n",
    "                    doc=\"document_\"\n",
    "                    name2=\"{}{}\".format(doc,index)\n",
    "                    result=[name2,res]\n",
    "                    dict2[dict[term]].append(result)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We stored this dictionary in index2.tsv file. Then we have the index needed for search engine 2 to get TfIdf for a specific word in a specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "\n",
    "\n",
    "with open('HW3 ADM/tsv/index2.tsv', 'w', newline='') as f_output:\n",
    "    tsv_index2 = csv.writer(f_output, delimiter='\\t')           \n",
    "    for key, val in dict2.items():\n",
    "        tsv_index2.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This 'getTfidf_query' function allows to calculate TfIdf value about input query. It's needed because we must calculate coisine similiarity between result document and input query (based on TfIdf).\n",
    "It has input query as parameter and returns a dataframe that has tfIdf for every word in input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def getTfidf_query(query):\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "    vectors = vectorizer.fit_transform([query])\n",
    "        #print(vectors)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df_query = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This 'getTfidf_document' function allows to get TfIdf about every word in query for a specific document.\n",
    "This function searches this value from index2 (that we have just created with TfIdf values). It has as parameters: input query and document where we got TfIdf fro every word in query for this document.\n",
    "It returns a dataframe with these matchings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfidf_document(words,document_id):\n",
    "    dict={}\n",
    "    \n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    #print(listWords)\n",
    "    df=pd.DataFrame(columns=listWords)\n",
    "    tfIdf=[]\n",
    "    with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        with open('HW3 ADM/tsv/index2.tsv', 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            \n",
    "        for word in listWords:\n",
    "            listDoc=[]\n",
    "            #print(\"word \" + word)\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    term=row[1]\n",
    "                    #print(\"teerm\" + str(term))\n",
    "                    break\n",
    "            for row in tsv_index:\n",
    "                if term==row[0]:\n",
    "\n",
    "                    listDoc=ast.literal_eval(row[1])\n",
    "\n",
    "                    #print(listDoc)\n",
    "                    break\n",
    "            for index in listDoc:\n",
    "\n",
    "                #print(index)\n",
    "                if index[0]==document_id:\n",
    "                    #print(index[0])\n",
    "\n",
    "                    tfIdf.append(index[1])\n",
    "                    #print(index[1])\n",
    "                    break\n",
    "\n",
    "        df.loc[0]=tfIdf\n",
    "        df=df.reindex(sorted(df.columns), axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'coisine' function allows to calculate coisine similairity from two list of TfIdf values. First list is about TfIdf values for input query calcuated by 'getTfidf_query' function. Second list is about TfIdf values for query input in document, calculated by 'getTfidf_document' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def coisine(list_query,list_document):\n",
    "    \n",
    "    res=(cosine_similarity([list_query,list_document]))\n",
    "    #print(res)\n",
    "    return res[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'searchEngine2' function allows to calculate the result for the second search engine.\n",
    "We call 'getDocuments(words,1)' to get all documents that contain input query.\n",
    "Then we call 'getTfidf_query(words)' to put in 'df' datframe all TfIdf values froi every word in input query.\n",
    "Then, for every document, we call 'getTfidf_document' and put its result in 'df_document' dataframe.\n",
    "Finally we call 'coisine' fucntion to get coisine value for every input and put its result in a final 'df' dataframe with other info(we get these infos from tsv file (intro, title, and url)). \n",
    "It's ordered by this similiarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def searchEngine2(words):\n",
    "    #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words,1)\n",
    "    if document=='No results':\n",
    "        return document\n",
    "    df_query=getTfidf_query(words)\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url','similarity'])\n",
    "\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        df_document=getTfidf_document(words,document[index])\n",
    "        \n",
    "        \n",
    "        #get Tfidf list from df_query dataframe\n",
    "        list_query=list(df_query.loc[0])\n",
    "        #get Tfidf list from df_document dataframe\n",
    "        list_document=list(df_document.loc[0])\n",
    "        \n",
    "        similiarity=coisine(list_document,list_query)\n",
    "        \n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        \n",
    "        url=totalMovies[int(numberDocument)]\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url,similiarity]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "            df=df.sort_values(by=['similarity'],ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22886</th>\n",
       "      <td>Paper Marriage</td>\n",
       "      <td>\\nPaper Marriage is a 1988 Hong Kong action co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Paper_Marriage</td>\n",
       "      <td>0.962736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21250</th>\n",
       "      <td>Candlestick</td>\n",
       "      <td>\\nCandlestick is a 2014 British film starring ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Candlestick_(film)</td>\n",
       "      <td>0.903011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20434</th>\n",
       "      <td>Four Weddings and a Funeral</td>\n",
       "      <td>\\nFour Weddings and a Funeral is a 1994 Britis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Four_Weddings_an...</td>\n",
       "      <td>0.698505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>Skyfall</td>\n",
       "      <td>\\nSkyfall is a 2012 British-American spy film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Skyfall</td>\n",
       "      <td>0.687316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23440</th>\n",
       "      <td>On the Job</td>\n",
       "      <td>\\nOn the Job (abbreviated OTJ) is a 2013 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/On_the_Job_(2013...</td>\n",
       "      <td>0.673693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26654</th>\n",
       "      <td>Simran</td>\n",
       "      <td>\\nSimran is a 2017 Indian heist crime drama fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Simran_(film)</td>\n",
       "      <td>0.666877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>Harry Potter and the Deathly Hallows – Part 2</td>\n",
       "      <td>\\nHarry Potter and the Deathly Hallows – Part ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Harry_Potter_and...</td>\n",
       "      <td>0.649515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25629</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>\\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Prince_of_Light</td>\n",
       "      <td>0.613451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "22886                                 Paper Marriage   \n",
       "21250                                    Candlestick   \n",
       "20434                    Four Weddings and a Funeral   \n",
       "21077                                        Skyfall   \n",
       "23440                                     On the Job   \n",
       "26654                                         Simran   \n",
       "20949  Harry Potter and the Deathly Hallows – Part 2   \n",
       "25629            Ramayana: The Legend of Prince Rama   \n",
       "\n",
       "                                                   intro  \\\n",
       "22886  \\nPaper Marriage is a 1988 Hong Kong action co...   \n",
       "21250  \\nCandlestick is a 2014 British film starring ...   \n",
       "20434  \\nFour Weddings and a Funeral is a 1994 Britis...   \n",
       "21077  \\nSkyfall is a 2012 British-American spy film ...   \n",
       "23440  \\nOn the Job (abbreviated OTJ) is a 2013 Phili...   \n",
       "26654  \\nSimran is a 2017 Indian heist crime drama fi...   \n",
       "20949  \\nHarry Potter and the Deathly Hallows – Part ...   \n",
       "25629  \\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...   \n",
       "\n",
       "                                                     url  similarity  \n",
       "22886       https://en.wikipedia.org/wiki/Paper_Marriage    0.962736  \n",
       "21250   https://en.wikipedia.org/wiki/Candlestick_(film)    0.903011  \n",
       "20434  https://en.wikipedia.org/wiki/Four_Weddings_an...    0.698505  \n",
       "21077              https://en.wikipedia.org/wiki/Skyfall    0.687316  \n",
       "23440  https://en.wikipedia.org/wiki/On_the_Job_(2013...    0.673693  \n",
       "26654        https://en.wikipedia.org/wiki/Simran_(film)    0.666877  \n",
       "20949  https://en.wikipedia.org/wiki/Harry_Potter_and...    0.649515  \n",
       "25629  https://en.wikipedia.org/wiki/The_Prince_of_Light    0.613451  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='In the United States 2019'\n",
    "\n",
    "searchEngine2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23440</th>\n",
       "      <td>On the Job</td>\n",
       "      <td>\\nOn the Job (abbreviated OTJ) is a 2013 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/On_the_Job_(2013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20434</th>\n",
       "      <td>Four Weddings and a Funeral</td>\n",
       "      <td>\\nFour Weddings and a Funeral is a 1994 Britis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Four_Weddings_an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21250</th>\n",
       "      <td>Candlestick</td>\n",
       "      <td>\\nCandlestick is a 2014 British film starring ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Candlestick_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>Harry Potter and the Deathly Hallows – Part 2</td>\n",
       "      <td>\\nHarry Potter and the Deathly Hallows – Part ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Harry_Potter_and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>Skyfall</td>\n",
       "      <td>\\nSkyfall is a 2012 British-American spy film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Skyfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22886</th>\n",
       "      <td>Paper Marriage</td>\n",
       "      <td>\\nPaper Marriage is a 1988 Hong Kong action co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Paper_Marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26654</th>\n",
       "      <td>Simran</td>\n",
       "      <td>\\nSimran is a 2017 Indian heist crime drama fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Simran_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25629</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>\\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Prince_of_Light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "23440                                     On the Job   \n",
       "20434                    Four Weddings and a Funeral   \n",
       "21250                                    Candlestick   \n",
       "20949  Harry Potter and the Deathly Hallows – Part 2   \n",
       "21077                                        Skyfall   \n",
       "22886                                 Paper Marriage   \n",
       "26654                                         Simran   \n",
       "25629            Ramayana: The Legend of Prince Rama   \n",
       "\n",
       "                                                   intro  \\\n",
       "23440  \\nOn the Job (abbreviated OTJ) is a 2013 Phili...   \n",
       "20434  \\nFour Weddings and a Funeral is a 1994 Britis...   \n",
       "21250  \\nCandlestick is a 2014 British film starring ...   \n",
       "20949  \\nHarry Potter and the Deathly Hallows – Part ...   \n",
       "21077  \\nSkyfall is a 2012 British-American spy film ...   \n",
       "22886  \\nPaper Marriage is a 1988 Hong Kong action co...   \n",
       "26654  \\nSimran is a 2017 Indian heist crime drama fi...   \n",
       "25629  \\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...   \n",
       "\n",
       "                                                     url  \n",
       "23440  https://en.wikipedia.org/wiki/On_the_Job_(2013...  \n",
       "20434  https://en.wikipedia.org/wiki/Four_Weddings_an...  \n",
       "21250   https://en.wikipedia.org/wiki/Candlestick_(film)  \n",
       "20949  https://en.wikipedia.org/wiki/Harry_Potter_and...  \n",
       "21077              https://en.wikipedia.org/wiki/Skyfall  \n",
       "22886       https://en.wikipedia.org/wiki/Paper_Marriage  \n",
       "26654        https://en.wikipedia.org/wiki/Simran_(film)  \n",
       "25629  https://en.wikipedia.org/wiki/The_Prince_of_Light  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchEngine1(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new score, Search Engine 3:\n",
    "###### We did 3rd search engine with \"zone index\" methods. We get info about this metohod from this link: https://moz.com/blog/search-engine-algorithm-basics.\n",
    "\n",
    "We conseidered the following sections for every movie: title, intro, plot and music. This method consists on assigning a fixed score. In our case we have:\n",
    "* title: 0.9\n",
    "* intro: 0.4\n",
    "* plot: 0.3\n",
    "* music: 0.6\n",
    "\n",
    "Then, we search all documents that contain input query (in title/intro/plot/music).\n",
    "For every document we get its score that we calculate in the following way:\n",
    "This score is a sum of different score in every section. You can sum the score for every section if and only if:\n",
    "* score about titile: if query contains the whole title;\n",
    "* score about intro/plot: if intro/plot contains the whole query;\n",
    "* score about music: if at least one word of the query is in music section.\n",
    "\n",
    "With this scoring we should give more importance to music and title section. \n",
    "We have choosen to give this score in the previous way about music because we want to sum this score when only name or surname (or both) of music compositor is in the input query. About title, we want to give this score becuase title must be totally in the input query.\n",
    "About intro and plot, we have given these score because we think that intro contains more significant words than words in plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created index for the search engine 3 and we put it in index3.tsv file. In this index, we considered also 'music' section to match document for every word. It's similiar to index1.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for row in tsv_vocabulary:\n",
    "            \n",
    "            dict2[row[1]]=[]\n",
    "        for index in range(len(totalMovies)):\n",
    "            \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                music=data_list[1][8]\n",
    "                music = ast.literal_eval(music)\n",
    "                text=plot+intro+music\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "                for i in text:\n",
    "                    for row in tsv_vocabulary:\n",
    "                        if i==row[0]:\n",
    "                            doc=\"document_\"\n",
    "                            name2=\"{}{}\".format(doc,index)\n",
    "                            \n",
    "                            dict2[row[1]].append(name2)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        #put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "        with open('HW3 ADM/tsv/index3.tsv', 'w', newline='') as f_output:\n",
    "            tsv_vocabulary = csv.writer(f_output, delimiter='\\t')           \n",
    "            for key, val in dict2.items():\n",
    "                tsv_vocabulary.writerow([key, val])           \n",
    "                    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function 'heapSortK' allows us to use a heap data structure (using heapq library) for maintaining the top-k documents. This function has a list and a k value. This value is the number of ordered results that we want to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import heapq\n",
    "def heappush(h, item, key=lambda x: x):\n",
    "    heapq.heappush(h, (key(item), item))\n",
    "\n",
    "def heappop(h):\n",
    "    return heapq.heappop(h)[1]\n",
    "\n",
    "def heapify(h, key=lambda x: x):\n",
    "    for idx, item in enumerate(h):\n",
    "        h[idx] = (key(item), item)\n",
    "    heapq.heapify(h)\n",
    "def heap3(a,k): \n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'plot', 'music','starring', 'score'])\n",
    "    result=[]\n",
    "    h = []\n",
    "    for item in a:\n",
    "        heappush(h, item, key=itemgetter(-1))\n",
    "    #print(h)\n",
    "    while h:\n",
    "        result.append(heappop(h))\n",
    "    \n",
    "    \n",
    "    j=0    #print(result)\n",
    "    result.reverse()\n",
    "    \n",
    "    for i in range(k):\n",
    "      \n",
    "        if len(a)>i:\n",
    "            \n",
    "            df.loc[j] = result[i]\n",
    "            j+=1\n",
    "        else:\n",
    "            break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 'search3' we have calculated the result for the 3rd search engine.\n",
    "Initially we have created a 'df_score' dataframe to store fixed score for every section (taht we considered). Then we call 'getDocuments(query,3)' to get all dcouments where all words in query are present on them (based on index3.tsv).\n",
    "\n",
    "For every document, we check the following aspects:\n",
    "* if all query words are in title section, sum score about title in 'df_score' to 'score' (final score);\n",
    "* if all query words are in intro section, sum score about intro in 'df_score' to 'score' (final score);\n",
    "* if all 'title_section' words are in input query, sum score about title in 'df_score' to 'score' (final score);\n",
    "* if at least one word in inout query is in 'music' section, sum score about music in 'df_score' to 'score' (final score);\n",
    "\n",
    "Final dataframe 'df' contains all movies searched with these following data (title,intro,plot,music, score). It's ordered by this score from 'heapFunction' function and it contains only first k movies. K is a value that the user has choosen. If there are less results than k, only available results are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "def search3(query,k):\n",
    "    \n",
    "    document=getDocuments(query,3)\n",
    "    if document=='No results':\n",
    "        return document\n",
    "    listWords = query.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'plot', 'music','starring', 'score'])\n",
    "    df_score=pd.DataFrame(columns=['title_score', 'intro_score', 'plot_score', 'music_score'])\n",
    "    scores=[0.8,0.4,0.3,0.6]\n",
    "    df_score.loc[0]=scores\n",
    "    resultMovies=[]\n",
    "    actors=[]\n",
    "    for index in range(len(document)):\n",
    "        score=0\n",
    "        lista=[]\n",
    "        #get id of documnets_is\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        listUrl_Movies3\n",
    "        url=totalMovies[int(numberDocument)]\n",
    "        \n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                    tsv_index = list(csv.reader(tsvfile, delimiter='\\t'))\n",
    "                    title=ast.literal_eval(tsv_index[1][3])\n",
    "                    \n",
    "                    intro=ast.literal_eval(tsv_index[1][1])\n",
    "\n",
    "                    plot=ast.literal_eval(tsv_index[1][2])\n",
    "                    \n",
    "                    music=ast.literal_eval(tsv_index[1][8])\n",
    "                    #actors.append(tsv_index[1][7])\n",
    "                    \n",
    "                    if (all(elem in title  for elem in listWords)) or (all(elem in listWords  for elem in title)):\n",
    "                            score+=df_score.loc[0]['title_score']\n",
    "                            \n",
    "                    if all(elem in intro  for elem in listWords)==True:\n",
    "                            score+=df_score.loc[0]['intro_score']\n",
    "                            \n",
    "                    if all(elem in plot  for elem in listWords)==True:\n",
    "                            score+=df_score.loc[0]['plot_score']\n",
    "                            \n",
    "                    if any(elem in music  for elem in listWords)==True:\n",
    "                        \n",
    "                        score+=df_score.loc[0]['music_score']\n",
    "                    \n",
    "\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_file = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title2=tsv_file[1][3]\n",
    "            intro2=tsv_file[1][1]\n",
    "            plot2=tsv_file[1][2]\n",
    "            music2=tsv_file[1][8]\n",
    "            listActors=ast.literal_eval(tsv_file[1][7])\n",
    "            actors=listActors\n",
    "            film=[title2,intro2,plot2,music2,actors,score]\n",
    "            resultMovies.append(film)\n",
    "            \n",
    "            \n",
    "       \n",
    "    df=heap3(resultMovies,k)\n",
    "    actorsGraph=[]\n",
    "    for index, row in df.iterrows():\n",
    "        actorsGraph.append(row['starring'])\n",
    "    #print(actorsGraph)\n",
    "    graph(actorsGraph)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Step: Make a nice visualization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this following function 'graph'we did the bonus section. This function has a nested list of actor, as parameter, about every film in results of search3. With this function we calculate evry couple of actors that are in (at least) two different film through film on the search3 result. Then we show these couple in a graph wehere every couple is represented by two nodes linked with an edge. If there is no actors couple, we don't show this graph.\n",
    "We used 'networkx' python library, as suggested in hw track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "def graph(listMovies):\n",
    "    G = nx.Graph() \n",
    "    couple=[]\n",
    "    count=0\n",
    "    for i in range(len(listMovies)):\n",
    "        for j in range(len(listMovies[i])):\n",
    "            for y in range(1,len(listMovies[i])):\n",
    "                couple=[listMovies[i][j],listMovies[i][y]]\n",
    "                count=0\n",
    "                for k in listMovies:\n",
    "                    if (couple[0] in k) and (couple[1] in k):\n",
    "                        \n",
    "                        count+=1\n",
    "                        if count>=2 and not couple[0] == couple[1]:\n",
    "                            \n",
    "                            G.add_edge(couple[0],couple[1])\n",
    "                            break\n",
    "     \n",
    " \n",
    "    \n",
    "    if not nx.is_empty(G):\n",
    "        pos = nx.spring_layout(G)   #<<<<<<<<<< Initialize this only once\n",
    "        nx.draw(G,pos=pos, with_labels=True, node_size = 100, font_size=10)\n",
    "        nx.draw_networkx_nodes(G,pos=pos, with_labels=True, node_size = 1500, font_size=10)\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.3)#<<<<<<<<< pass the pos variable\n",
    "        #plt.draw() \n",
    "        plt.figure(figsize=(8, 8))  # image is 8 x 8 inches\n",
    "         # To plot the next graph in a new figure\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>plot</th>\n",
       "      <th>music</th>\n",
       "      <th>starring</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Four Weddings and a Funeral</td>\n",
       "      <td>\\nFour Weddings and a Funeral is a 1994 Britis...</td>\n",
       "      <td>At the wedding of Angus and Laura in Somerset,...</td>\n",
       "      <td>Richard Rodney Bennett</td>\n",
       "      <td>[Hugh Grant, Andie MacDowell, Kristin Scott Th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tree of Hands</td>\n",
       "      <td>\\nTree of Hands (also known as Innocent Victim...</td>\n",
       "      <td>Benet Archdale (Helen Shaver), a London based ...</td>\n",
       "      <td>Richard Hartley</td>\n",
       "      <td>[Helen Shaver, Lauren Bacall, Malcolm Stoddard...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pouvoir intime</td>\n",
       "      <td>Pouvoir intime (French pronunciation: ​[puvwaʁ...</td>\n",
       "      <td>A government ministry's fast-rising head of se...</td>\n",
       "      <td>Richard Grégoire</td>\n",
       "      <td>[Jean-Louis Millette, Eric Brisebois, Marie Ti...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iron Monkey</td>\n",
       "      <td>\\nIron Monkey is a 1993 Hong Kong martial arts...</td>\n",
       "      <td>The plot centers on a masked martial artist kn...</td>\n",
       "      <td>Richard Yuen (Hong Kong)James L. Venable(Unite...</td>\n",
       "      <td>[Donnie Yen, Yu Rongguang, Angie Tsang]</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Imagine You and Me</td>\n",
       "      <td>Imagine You and Me (stylized as Imagine You &amp; ...</td>\n",
       "      <td>Gara (Maine Mendoza) is an OFW who works very ...</td>\n",
       "      <td>Richard Gonzales\\nJay Dominguez</td>\n",
       "      <td>[Alden Richards, Maine Mendoza]</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Heat and Dust</td>\n",
       "      <td>\\nHeat and Dust is a 1983 romantic drama film ...</td>\n",
       "      <td>In 1982, an Englishwoman named Anne (Julie Chr...</td>\n",
       "      <td>Richard RobbinsZakir Hussain</td>\n",
       "      <td>[Shashi Kapoor, Greta Scacchi, Julie Christie,...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  \\\n",
       "0  Four Weddings and a Funeral   \n",
       "1                Tree of Hands   \n",
       "2               Pouvoir intime   \n",
       "3                  Iron Monkey   \n",
       "4           Imagine You and Me   \n",
       "5                Heat and Dust   \n",
       "\n",
       "                                               intro  \\\n",
       "0  \\nFour Weddings and a Funeral is a 1994 Britis...   \n",
       "1  \\nTree of Hands (also known as Innocent Victim...   \n",
       "2  Pouvoir intime (French pronunciation: ​[puvwaʁ...   \n",
       "3  \\nIron Monkey is a 1993 Hong Kong martial arts...   \n",
       "4  Imagine You and Me (stylized as Imagine You & ...   \n",
       "5  \\nHeat and Dust is a 1983 romantic drama film ...   \n",
       "\n",
       "                                                plot  \\\n",
       "0  At the wedding of Angus and Laura in Somerset,...   \n",
       "1  Benet Archdale (Helen Shaver), a London based ...   \n",
       "2  A government ministry's fast-rising head of se...   \n",
       "3  The plot centers on a masked martial artist kn...   \n",
       "4  Gara (Maine Mendoza) is an OFW who works very ...   \n",
       "5  In 1982, an Englishwoman named Anne (Julie Chr...   \n",
       "\n",
       "                                               music  \\\n",
       "0                             Richard Rodney Bennett   \n",
       "1                                    Richard Hartley   \n",
       "2                                   Richard Grégoire   \n",
       "3  Richard Yuen (Hong Kong)James L. Venable(Unite...   \n",
       "4                    Richard Gonzales\\nJay Dominguez   \n",
       "5                       Richard RobbinsZakir Hussain   \n",
       "\n",
       "                                            starring  score  \n",
       "0  [Hugh Grant, Andie MacDowell, Kristin Scott Th...    1.0  \n",
       "1  [Helen Shaver, Lauren Bacall, Malcolm Stoddard...    0.6  \n",
       "2  [Jean-Louis Millette, Eric Brisebois, Marie Ti...    0.6  \n",
       "3            [Donnie Yen, Yu Rongguang, Angie Tsang]    0.6  \n",
       "4                    [Alden Richards, Maine Mendoza]    0.6  \n",
       "5  [Shashi Kapoor, Greta Scacchi, Julie Christie,...    0.6  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='United States Richard'\n",
    "search3(query,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>plot</th>\n",
       "      <th>music</th>\n",
       "      <th>starring</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Man with Rain in His Shoes</td>\n",
       "      <td>\\nThe Man with Rain in His Shoes is a 1998 Spa...</td>\n",
       "      <td>Victor (Henshall) is an actor in London who is...</td>\n",
       "      <td>NA</td>\n",
       "      <td>[Lena Headey, Douglas Henshall, Penélope Cruz,...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Imitation Game</td>\n",
       "      <td>\\nThe Imitation Game is a 2014 American histor...</td>\n",
       "      <td>In 1951, two policemen, Nock and Staehl, inves...</td>\n",
       "      <td>Alexandre Desplat</td>\n",
       "      <td>[Benedict Cumberbatch, Keira Knightley, Matthe...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grimsby</td>\n",
       "      <td>\\nGrimsby (released in the United States as Th...</td>\n",
       "      <td>\"Nobby\" Butcher has been separated from his li...</td>\n",
       "      <td>Erran Baron Cohen\\nDavid Buckley</td>\n",
       "      <td>[Mark Strong, Rebel Wilson, Penélope Cruz, Isl...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Before I Go to Sleep</td>\n",
       "      <td>\\nBefore I Go to Sleep is a 2014 mystery psych...</td>\n",
       "      <td>Forty-year-old Christine Lucas wakes up in bed...</td>\n",
       "      <td>Edward Shearmur</td>\n",
       "      <td>[Nicole Kidman, Mark Strong, Colin Firth, Anne...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0  The Man with Rain in His Shoes   \n",
       "1              The Imitation Game   \n",
       "2                         Grimsby   \n",
       "3            Before I Go to Sleep   \n",
       "\n",
       "                                               intro  \\\n",
       "0  \\nThe Man with Rain in His Shoes is a 1998 Spa...   \n",
       "1  \\nThe Imitation Game is a 2014 American histor...   \n",
       "2  \\nGrimsby (released in the United States as Th...   \n",
       "3  \\nBefore I Go to Sleep is a 2014 mystery psych...   \n",
       "\n",
       "                                                plot  \\\n",
       "0  Victor (Henshall) is an actor in London who is...   \n",
       "1  In 1951, two policemen, Nock and Staehl, inves...   \n",
       "2  \"Nobby\" Butcher has been separated from his li...   \n",
       "3  Forty-year-old Christine Lucas wakes up in bed...   \n",
       "\n",
       "                              music  \\\n",
       "0                                NA   \n",
       "1                 Alexandre Desplat   \n",
       "2  Erran Baron Cohen\\nDavid Buckley   \n",
       "3                   Edward Shearmur   \n",
       "\n",
       "                                            starring  score  \n",
       "0  [Lena Headey, Douglas Henshall, Penélope Cruz,...    0.4  \n",
       "1  [Benedict Cumberbatch, Keira Knightley, Matthe...    0.4  \n",
       "2  [Mark Strong, Rebel Wilson, Penélope Cruz, Isl...    0.4  \n",
       "3  [Nicole Kidman, Mark Strong, Colin Firth, Anne...    0.4  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='United States Strong Mark '\n",
    "search3(query,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
