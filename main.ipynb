{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection:\n",
    "\n",
    "##  Get the list of movies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get a list of Wikipedia URLs from movies1.html/ movies2.html/ movies3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "f1 = open(\"HW3 ADM/movies3.html\")\n",
    "f3 = open(\"HW3 ADM/movies1.html\")\n",
    "f2 = open(\"HW3 ADM/movies2.html\")\n",
    "soup = BeautifulSoup(f1)\n",
    "soup1 = BeautifulSoup(f3)\n",
    "soup2 = BeautifulSoup(f2)\n",
    "listUrl_Movies1=[]\n",
    "listUrl_Movies2=[]\n",
    "listUrl_Movies3=[]\n",
    "for link in soup.select('a'):\n",
    "    listUrl_Movies3.append(link.text)\n",
    "for link in soup2.select('a'):\n",
    "    listUrl_Movies2.append(link.text)\n",
    "for link in soup1.select('a'):\n",
    "    listUrl_Movies1.append(link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge these 3 lists in a single list that contains all 30 000 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalMovies=listUrl_Movies1+listUrl_Movies2+listUrl_Movies3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Wikipedia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function 'downloadFile' that allows to download html files from URL list\n",
    "\n",
    "We downloaded all movies. 10 000 per person. Every person downloaded file from:\n",
    "* listUrl_Movies1/ listUrl_Movies2/ listUrl_Movies3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile():\n",
    "\n",
    "    for index in range(listUrl_Movies1):\n",
    "        #set time=20 mintes\n",
    "        t2=1200\n",
    "        try:\n",
    "            #wait 5 seconds bettween every request\n",
    "            t1 = random.randint(1,5)\n",
    "            time.sleep(t1)\n",
    "            url=listUrl_Movies3[index]\n",
    "            response = requests.get(url)\n",
    "            name=\"article_\"\n",
    "            extension=\".html\"\n",
    "            file=\"{}{}{}\".format(name,index,extension)\n",
    "            with open(file,'wb') as f: \n",
    "                f.write(response.content)  \n",
    "\n",
    "        except response.status_code as e:\n",
    "            print(\"exception\")\n",
    "            #error=492 is error that occurs when we have done a limit of request\n",
    "            if e==492:\n",
    "                #wait 20 minutes \n",
    "                time.sleep(t2)\n",
    "                downloadFile(index+1)\n",
    "            elif e==200:\n",
    "                soup = BeautifulSoup(listUrl_Movies3[1])\n",
    "                \n",
    "                with open(file,'w') as f: \n",
    "                    f.write(soup.text)\n",
    "                downloadFile(index+1)\n",
    "            else:\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse downloaded pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create .tsv files for every html file and put them in tsv directory. These files contain data (title,intro,plot and infobox infos) as written in hw track. About starring in infobox, we saved every actor in a list (to do bonus section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os.path\n",
    "#define column of our dataframe\n",
    "df=pd.DataFrame(columns=['title', 'intro', 'plot','film_name','producer','director','writer','starring','music','release date','runtime','country','language','budget'])\n",
    "\n",
    "\n",
    "for index in range(len(totalMovies)):\n",
    "    print(index)\n",
    "    title=''\n",
    "    plot=''\n",
    "    intro=''\n",
    "    title_name='NA'\n",
    "    producer='NA'\n",
    "    director='NA'\n",
    "    writer='NA'\n",
    "    starring=['NA']\n",
    "    music='NA'\n",
    "    release_date='NA'\n",
    "    runtime='NA'\n",
    "    country='NA'\n",
    "    language='NA'\n",
    "    budget='NA'\n",
    "    \n",
    "    \n",
    "    #define name of the file that we want to find (in my case: in the same directory)\n",
    "    name=\"article_\"\n",
    "    extension=\".html\"\n",
    "    file=\"{}{}{}\".format(name,index ,extension)\n",
    "    \n",
    "    #check if this file exists\n",
    "    if not os.path.isfile(\"HW3 ADM/\"+file):\n",
    "        continue\n",
    "        \n",
    "    #open file   \n",
    "    response2 = open(\"HW3 ADM/\"+file)\n",
    "    soup = BeautifulSoup(response2)\n",
    "    #take title.\n",
    "    title=soup.title.text.rsplit(' ', 2)[0]\n",
    "    \n",
    "    #take all p in intro(firt section)\n",
    "    #print(soup.find('span', attrs={'class': 'mw-headline'}))\n",
    "    if soup.find('span', attrs={'class': 'mw-headline'}):\n",
    "        heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "        paragraphs = heading.find_all_previous('p')\n",
    "        for p in paragraphs: \n",
    "            intro = p.text + intro\n",
    "            \n",
    "     \n",
    "        #take all p in 'plot'(second section)\n",
    "        b=True\n",
    "        #print(soup.find('span', attrs={'class': 'mw-headline'}))\n",
    "        if soup.find('span', attrs={'class': 'mw-headline'}): \n",
    "            \n",
    "            heading = soup.find('span', attrs={'class': 'mw-headline'})\n",
    "            \n",
    "            for item in heading.parent.nextSiblingGenerator():\n",
    "                \n",
    "                if item.name=='h2':\n",
    "                    break\n",
    "                if hasattr(item, \"text\"):\n",
    "                    \n",
    "                    plot+=item.text\n",
    "\n",
    "        else:\n",
    "            plot=\"NAN\"\n",
    "    \n",
    "    else:\n",
    "        intro=\"NAN\"\n",
    "        plot=\"NAN\"\n",
    "    \n",
    "    \n",
    "    #Get info about infobox from every page and put them in respective sections in tsv file  \n",
    "    if soup.find('table', attrs={'class': 'infobox vevent'}):\n",
    "        \n",
    "        table = soup.find('table', attrs={'class': 'infobox vevent'})  \n",
    "    \n",
    "        if table.find('th', attrs={'class': 'summary'}):\n",
    "        \n",
    "            x=table.find('th', attrs={'class': 'summary'})\n",
    "            title_name=x.text.strip()\n",
    "        \n",
    "        for cell in table.find_all('th'):\n",
    "        \n",
    "            if cell.find_next_sibling('td'):\n",
    "                a=cell.find_next_sibling('td')\n",
    "                if cell.text.strip()=='Directed by':\n",
    "                    director=a.text.strip()\n",
    "                elif cell.text.strip()=='Produced by':\n",
    "                \n",
    "                    producer=a.text.strip()\n",
    "                elif cell.text.strip()=='Written by':\n",
    "                \n",
    "                    writer=a.text.strip()\n",
    "                elif cell.text.strip()=='Starring':\n",
    "                    listStarring=[]\n",
    "                    for link in a.select('a'):\n",
    "                        \n",
    "                        listStarring.append(link.text)\n",
    "                    starring=listStarring\n",
    "                    #print(starring)\n",
    "                elif cell.text.strip()=='Music by':\n",
    "                \n",
    "                    music=a.text.strip()\n",
    "                elif cell.text.strip()=='Release date':\n",
    "                    release_date=a.text.strip()   \n",
    "                elif cell.text.strip()=='Running time':\n",
    "                \n",
    "                    runtime=a.text.strip()\n",
    "                elif cell.text.strip()=='Country':\n",
    "              \n",
    "                    country=a.text.strip()\n",
    "                elif cell.text.strip()=='Language':\n",
    "              \n",
    "                    language=a.text.strip()\n",
    "                elif cell.text.strip()=='Budget':\n",
    "              \n",
    "                    budget=a.text.strip()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    \n",
    "    #put all infos in movie list\n",
    "    movie=[title,intro,plot,title_name,producer,director,writer,starring,music,release_date,runtime,country,language,budget]\n",
    "    #update dataframe with this list\n",
    "    extension2=\".tsv\"\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "   \n",
    "    movieTitle=[\"title\",\"intro\",\"plot\",\"title_name\",\"producer\",\"director\",\"writer\",\"starring\",\"music\",\"release_date\",\"runtime\",\"country\",\"language\",\"budget\"]\n",
    "    with open(\"HW3 ADM/tsv_new/\"+file, 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_output.writerow(movieTitle)\n",
    "        tsv_output.writerow(movie)\n",
    "    df.loc[index] = movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine:\n",
    "\n",
    "## Search engine 1: Conjunctive query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create tsv files with preporcessed text. We have preprocessed all texts in tsv file (that we have just created) and we have put them in other tsv files in tsv_correct directory. For every section we have a list of all words that are in respective section. We have also deleted duplicated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import csv\n",
    "\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(len(totalMovies)):\n",
    "    print(index)\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv/\"+file,\"r\") as tsvfile, open(\"HW3 ADM/tsv_correct/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(set(map(str.lower, row[i])))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created a vocabulary that we stored in vocabulary.tsv file. Here we have all words that we have in all tsv files (in intro and plot section as wirtten in hw track). Every word matches with an unique term_id. This term_is is a simple counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "#create vocabulary and save it on vocabulary.tsv\n",
    "            \n",
    "\n",
    "dict1 = dict()\n",
    "term_id=0\n",
    "present=False\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'w', newline='') as f_output:\n",
    "        tsv_vocabulary = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_vocabulary.writerow(['word','term_id'])\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        \n",
    "        for index in range(len(totalMovies)):\n",
    "           \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                #put in intro a list of all words that we have in intro of i-th page\n",
    "                intro=data_list[1][2]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                #put in plot a list of all words that we have in plot of i-th page\n",
    "                plot=data_list[1][1]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                \n",
    "                #put in text, a list that contains all words that are in plot and word for every page (no duplicate)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #put in dict1 every words with its term_id (no duplicate)\n",
    "                for i in text:\n",
    "                    if i in dict1:    \n",
    "                        continue\n",
    "                    else:\n",
    "                        dict1[i]=term_id\n",
    "                        term_id+=1\n",
    "                \n",
    "        #put dict1 element in vocabulary.tsv file                \n",
    "        for key, val in dict1.items():\n",
    "                    tsv_vocabulary.writerow([key, val])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we created the index from vocabulary.tsv and from all preprocessed tsv files.  \n",
    "For every word (in plot and intro) in all prorocessed tsv files, we found their term_id (from vocabulary) and for evey term_id we match a list of document where repsective word is present. \n",
    "* This index was used to search query words in every document about 1st and 2nd search engines. We saved it into index.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for row in tsv_vocabulary:\n",
    "            \n",
    "            dict2[row[1]]=[]\n",
    "        for index in range(len(totalMovies)):\n",
    "            \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "                for i in text:\n",
    "                    for row in tsv_vocabulary:\n",
    "                        if i==row[0]:\n",
    "                            doc=\"document_\"\n",
    "                            name2=\"{}{}\".format(doc,index)\n",
    "                            \n",
    "                            dict2[row[1]].append(name2)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        #put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "        with open('HW3 ADM/tsv/index1.tsv', 'w', newline='') as f_output:\n",
    "            tsv_vocabulary = csv.writer(f_output, delimiter='\\t')           \n",
    "            for key, val in dict2.items():\n",
    "                tsv_vocabulary.writerow([key, val])           \n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we create getDocuments function that allows to find all documents where the input query is present.\n",
    "This fucntion has two parameters in input: words that is our query and index is the number of index that we consider to do specific search engine. \n",
    "This function returns a list of documents that contains input query based on index.\n",
    "* We use index1 about search engine 1 and search engine 2; index2 about search engine 3.\n",
    "\n",
    "* This function is used for all three different search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "#define function that allows us to calculate a list that is an intersection from two list\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "def getDocuments(words,index):\n",
    "\n",
    "\n",
    "    #we use dict3 to store term_id and its respective documents_id\n",
    "    dict3={}\n",
    "    ##we use dict4 to store evry word and its respective documents_id\n",
    "    dict4={}\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    #in listWords we have a list that contains all words about inout query\n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    \n",
    "    #with vocabulary.tsv we start to build a dict3 with term_id for every words in wordsList\n",
    "    with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for word in listWords:\n",
    "            word=word.lower()\n",
    "            present=False\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    dict3[row[1]]=[]\n",
    "                    present=True\n",
    "            #case where word is not in vocabulary\n",
    "            if present==False:\n",
    "                dict4[word]=[]\n",
    "        indexFile=\"index\"+str(index)+\".tsv\"\n",
    "        #we continue to match documnets_id to every term_id in dict3\n",
    "        with open('HW3 ADM/tsv/'+indexFile, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            for k in dict3.keys():  \n",
    "                for row in tsv_index:\n",
    "                    if row[0]==k:\n",
    "                        dict3[k]=row[1]\n",
    "                        continue\n",
    "\n",
    "\n",
    "        #finally we build dict4 where evry word matches to respective documents_id\n",
    "        for k in dict3.keys():\n",
    "\n",
    "            for row in tsv_vocabulary:\n",
    "                if k==row[1]:\n",
    "                    dict4[row[0]]=dict3[row[1]]\n",
    "\n",
    "        document=ast.literal_eval(dict4[listWords[0]])         \n",
    "        #return \"no results\" if any query words isn't at least in one document\n",
    "        for i in dict4.values():\n",
    "            if not i:\n",
    "                error='No results'\n",
    "                return error\n",
    "        #interection between every list in values dict4. In this way we have documnets_id where all words (in query input) are present\n",
    "        for value in dict4.values():\n",
    "            document=intersection(document,ast.literal_eval(value))\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* searchEngine1 function allows to do the first search engine. It takes an input query and returns a dataframe that contains the result. In this function we call getDocuments and for every document that contains input query we get some info from not preprocessed tsv file in tsv directory.\n",
    "\n",
    "* This result is a list of movies and for every movies we show only intro, title and URL link.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def searchEngine1(words):\n",
    "        #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words,1)\n",
    "    if document=='No results':\n",
    "        \n",
    "        return document\n",
    "    elif document==[]:\n",
    "        return \"No results\"\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url'])\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        \n",
    "        url=totalMovies[int(numberDocument)]\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20012</th>\n",
       "      <td>Digby, the Biggest Dog in the World</td>\n",
       "      <td>\\nDigby, the Biggest Dog in the World is the t...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Digby,_the_Bigge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title  \\\n",
       "20012  Digby, the Biggest Dog in the World   \n",
       "\n",
       "                                                   intro  \\\n",
       "20012  \\nDigby, the Biggest Dog in the World is the t...   \n",
       "\n",
       "                                                     url  \n",
       "20012  https://en.wikipedia.org/wiki/Digby,_the_Bigge...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='enormous damage unless something is done immediately'\n",
    "searchEngine1(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search engine 2: Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create tsv files from not preprocessed files. This is the same process that we've done for tsv file in tsv_correct dirctory. But in this case, we considerated also duplicate words. This it is important to calculate TfIdf about second search engine.\n",
    "\n",
    "* These tsv file are stored in tsv_correct2 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import string\n",
    "import csv\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#create tsv file in 'tsv_correct' directory wehere we have preprocessed the tsv file (just created in parser.py)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(len(totalMovies)):\n",
    "    print(index)\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv/\"+file,\"r\") as tsvfile, open(\"HW3 ADM/tsv_correct2/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(map(str.lower, row[i]))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From these tsv file that we have just created, we create a dataframe 'df2'.\n",
    "This dataframe contains the TfIdf value for every match between word-document. Its columns are all different words that we have preproccesed tsv file (in intro and plot). Its rows are different document that are identified by document_id (id stays for the number of movies, example:document_222 stays for article_222.html/article_222.tsv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "documents=[]\n",
    "name='article_'\n",
    "extension2='.tsv'\n",
    "with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for index in range(len(totalMovies)):\n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text=list(map(str.lower, text))\n",
    "                documents.append(' '.join(text))\n",
    "                #print(documents)\n",
    "                # text2= list(set(map(str.lower, text)))\n",
    "                \n",
    "                \n",
    "        vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "        vectors = vectorizer.fit_transform(documents)\n",
    "        #print(vectors)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        df2 = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created a new dictionary that has term_id as key (that we get from vocabulary by every word) and an array as value. This array contains a list of matching between document and respective TfIdf for respective word in this document. For example:\n",
    "key: \"122\", value:[[dcoument_12,0.02],[dcoument_18,0.22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "dict={}\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "\n",
    "\n",
    "#print('step 2')\n",
    "name=\"article_\"\n",
    "extension2=\".tsv\"\n",
    "h=0\n",
    "for row in tsv_vocabulary:\n",
    "    h+=1\n",
    "    dict[row[0]]=row[1]\n",
    "    print(h)\n",
    "    dict2[row[1]]=[]\n",
    "for index in range(len(totalMovies)):\n",
    "    #print(index)\n",
    "    print(\"numero documento \"+ str(index))\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"HW3 ADM/tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "        data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        intro=data_list[1][1]\n",
    "        intro = ast.literal_eval(intro)\n",
    "        plot=data_list[1][2]\n",
    "        plot = ast.literal_eval(plot)\n",
    "        text=plot+intro\n",
    "        text=list(map(str.lower, text))\n",
    "    \n",
    "        text2= list(set(map(str.lower, text)))\n",
    "        #print(text2)\n",
    "                        #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "        for i in text2:\n",
    "\n",
    "                            #print(\"word   \"+str(i))\n",
    "            # print(\"word \"+str(i))\n",
    "            # print(\"aaa\" + str(df2.iloc[index][i]))\n",
    "            res=df2.iloc[index][i]\n",
    "                            #print(\"res   \"+str(res))\n",
    "            for term in dict:\n",
    "                if i==term:\n",
    "                    #print(\"key \"+ str(term))\n",
    "                    #print(\"value \"+ str(dict[term]))\n",
    "                    doc=\"document_\"\n",
    "                    name2=\"{}{}\".format(doc,index)\n",
    "                    result=[name2,res]\n",
    "                    dict2[dict[term]].append(result)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We stored this dictionary in index2.tsv file. Then we have the index needed for search engine 2 to get TfIdf for a specific word in a specific document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "\n",
    "\n",
    "with open('HW3 ADM/tsv/index2.tsv', 'w', newline='') as f_output:\n",
    "    tsv_index2 = csv.writer(f_output, delimiter='\\t')           \n",
    "    for key, val in dict2.items():\n",
    "        tsv_index2.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This 'getTfidf_query' function allows to calculate TfIdf value about input query. It's needed because we must calculate coisine similiarity between result document and input query (based on TfIdf).\n",
    "It has input query as parameter and returns a dataframe that has tfIdf for every word in input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def getTfidf_query(query):\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "    vectors = vectorizer.fit_transform([query])\n",
    "        #print(vectors)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df_query = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This 'getTfidf_document' function allows to get TfIdf about every word in query for a specific document.\n",
    "This function searches this value from index2 (that we have just created with TfIdf values). It has as parameters: input query and document where we got TfIdf fro every word in query for this document.\n",
    "It returns a dataframe with these matchings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTfidf_document(words,document_id):\n",
    "    dict={}\n",
    "    \n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    #print(listWords)\n",
    "    df=pd.DataFrame(columns=listWords)\n",
    "    tfIdf=[]\n",
    "    with open('HW3 ADM/tsv/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        with open('HW3 ADM/tsv/index2.tsv', 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            \n",
    "        for word in listWords:\n",
    "            listDoc=[]\n",
    "            #print(\"word \" + word)\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    term=row[1]\n",
    "                    #print(\"teerm\" + str(term))\n",
    "                    break\n",
    "            for row in tsv_index:\n",
    "                if term==row[0]:\n",
    "\n",
    "                    listDoc=ast.literal_eval(row[1])\n",
    "\n",
    "                    #print(listDoc)\n",
    "                    break\n",
    "            for index in listDoc:\n",
    "\n",
    "                #print(index)\n",
    "                if index[0]==document_id:\n",
    "                    #print(index[0])\n",
    "\n",
    "                    tfIdf.append(index[1])\n",
    "                    #print(index[1])\n",
    "                    break\n",
    "\n",
    "        df.loc[0]=tfIdf\n",
    "        df=df.reindex(sorted(df.columns), axis=1)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'coisine' function allows to calculate coisine similairity from two list of TfIdf values. First list is about TfIdf values for input query calcuated by 'getTfidf_query' function. Second list is about TfIdf values for query input in document, calculated by 'getTfidf_document' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def coisine(list_query,list_document):\n",
    "    \n",
    "    res=(cosine_similarity([list_query,list_document]))\n",
    "    #print(res)\n",
    "    return res[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'searchEngine2' function allows to calculate the result for the second search engine.\n",
    "We call 'getDocuments(words,1)' to get all documents that contain input query.\n",
    "Then we call 'getTfidf_query(words)' to put in 'df' datframe all TfIdf values froi every word in input query.\n",
    "Then, for every document, we call 'getTfidf_document' and put its result in 'df_document' dataframe.\n",
    "Finally we call 'coisine' fucntion to get coisine value for every input and put its result in a final 'df' dataframe with other info(we get these infos from tsv file (intro, title, and url)). \n",
    "It's ordered by this similiarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def searchEngine2(words):\n",
    "    #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words,1)\n",
    "    if document=='No results':\n",
    "        \n",
    "        return document\n",
    "    elif document==[]:\n",
    "        return \"No results\"\n",
    "    df_query=getTfidf_query(words)\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url','similarity'])\n",
    "\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        df_document=getTfidf_document(words,document[index])\n",
    "        \n",
    "        \n",
    "        #get Tfidf list from df_query dataframe\n",
    "        list_query=list(df_query.loc[0])\n",
    "        #get Tfidf list from df_document dataframe\n",
    "        list_document=list(df_document.loc[0])\n",
    "        \n",
    "        similiarity=coisine(list_document,list_query)\n",
    "        \n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        \n",
    "        url=totalMovies[int(numberDocument)]\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url,similiarity]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "            df=df.sort_values(by=['similarity'],ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22886</th>\n",
       "      <td>Paper Marriage</td>\n",
       "      <td>\\nPaper Marriage is a 1988 Hong Kong action co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Paper_Marriage</td>\n",
       "      <td>0.962736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21250</th>\n",
       "      <td>Candlestick</td>\n",
       "      <td>\\nCandlestick is a 2014 British film starring ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Candlestick_(film)</td>\n",
       "      <td>0.903011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20434</th>\n",
       "      <td>Four Weddings and a Funeral</td>\n",
       "      <td>\\nFour Weddings and a Funeral is a 1994 Britis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Four_Weddings_an...</td>\n",
       "      <td>0.698505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>Skyfall</td>\n",
       "      <td>\\nSkyfall is a 2012 British-American spy film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Skyfall</td>\n",
       "      <td>0.687316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23440</th>\n",
       "      <td>On the Job</td>\n",
       "      <td>\\nOn the Job (abbreviated OTJ) is a 2013 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/On_the_Job_(2013...</td>\n",
       "      <td>0.673693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26654</th>\n",
       "      <td>Simran</td>\n",
       "      <td>\\nSimran is a 2017 Indian heist crime drama fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Simran_(film)</td>\n",
       "      <td>0.666877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>Harry Potter and the Deathly Hallows – Part 2</td>\n",
       "      <td>\\nHarry Potter and the Deathly Hallows – Part ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Harry_Potter_and...</td>\n",
       "      <td>0.649515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25629</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>\\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Prince_of_Light</td>\n",
       "      <td>0.613451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "22886                                 Paper Marriage   \n",
       "21250                                    Candlestick   \n",
       "20434                    Four Weddings and a Funeral   \n",
       "21077                                        Skyfall   \n",
       "23440                                     On the Job   \n",
       "26654                                         Simran   \n",
       "20949  Harry Potter and the Deathly Hallows – Part 2   \n",
       "25629            Ramayana: The Legend of Prince Rama   \n",
       "\n",
       "                                                   intro  \\\n",
       "22886  \\nPaper Marriage is a 1988 Hong Kong action co...   \n",
       "21250  \\nCandlestick is a 2014 British film starring ...   \n",
       "20434  \\nFour Weddings and a Funeral is a 1994 Britis...   \n",
       "21077  \\nSkyfall is a 2012 British-American spy film ...   \n",
       "23440  \\nOn the Job (abbreviated OTJ) is a 2013 Phili...   \n",
       "26654  \\nSimran is a 2017 Indian heist crime drama fi...   \n",
       "20949  \\nHarry Potter and the Deathly Hallows – Part ...   \n",
       "25629  \\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...   \n",
       "\n",
       "                                                     url  similarity  \n",
       "22886       https://en.wikipedia.org/wiki/Paper_Marriage    0.962736  \n",
       "21250   https://en.wikipedia.org/wiki/Candlestick_(film)    0.903011  \n",
       "20434  https://en.wikipedia.org/wiki/Four_Weddings_an...    0.698505  \n",
       "21077              https://en.wikipedia.org/wiki/Skyfall    0.687316  \n",
       "23440  https://en.wikipedia.org/wiki/On_the_Job_(2013...    0.673693  \n",
       "26654        https://en.wikipedia.org/wiki/Simran_(film)    0.666877  \n",
       "20949  https://en.wikipedia.org/wiki/Harry_Potter_and...    0.649515  \n",
       "25629  https://en.wikipedia.org/wiki/The_Prince_of_Light    0.613451  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='In the United States 2019'\n",
    "\n",
    "searchEngine2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>Harry Potter and the Deathly Hallows – Part 2</td>\n",
       "      <td>\\nHarry Potter and the Deathly Hallows – Part ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Harry_Potter_and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23440</th>\n",
       "      <td>On the Job</td>\n",
       "      <td>\\nOn the Job (abbreviated OTJ) is a 2013 Phili...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/On_the_Job_(2013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21250</th>\n",
       "      <td>Candlestick</td>\n",
       "      <td>\\nCandlestick is a 2014 British film starring ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Candlestick_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25629</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>\\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Prince_of_Light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26654</th>\n",
       "      <td>Simran</td>\n",
       "      <td>\\nSimran is a 2017 Indian heist crime drama fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Simran_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>Skyfall</td>\n",
       "      <td>\\nSkyfall is a 2012 British-American spy film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Skyfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22886</th>\n",
       "      <td>Paper Marriage</td>\n",
       "      <td>\\nPaper Marriage is a 1988 Hong Kong action co...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Paper_Marriage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20434</th>\n",
       "      <td>Four Weddings and a Funeral</td>\n",
       "      <td>\\nFour Weddings and a Funeral is a 1994 Britis...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Four_Weddings_an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "20949  Harry Potter and the Deathly Hallows – Part 2   \n",
       "23440                                     On the Job   \n",
       "21250                                    Candlestick   \n",
       "25629            Ramayana: The Legend of Prince Rama   \n",
       "26654                                         Simran   \n",
       "21077                                        Skyfall   \n",
       "22886                                 Paper Marriage   \n",
       "20434                    Four Weddings and a Funeral   \n",
       "\n",
       "                                                   intro  \\\n",
       "20949  \\nHarry Potter and the Deathly Hallows – Part ...   \n",
       "23440  \\nOn the Job (abbreviated OTJ) is a 2013 Phili...   \n",
       "21250  \\nCandlestick is a 2014 British film starring ...   \n",
       "25629  \\nRamayana: The Legend of Prince Rama (ラーマヤーナ ...   \n",
       "26654  \\nSimran is a 2017 Indian heist crime drama fi...   \n",
       "21077  \\nSkyfall is a 2012 British-American spy film ...   \n",
       "22886  \\nPaper Marriage is a 1988 Hong Kong action co...   \n",
       "20434  \\nFour Weddings and a Funeral is a 1994 Britis...   \n",
       "\n",
       "                                                     url  \n",
       "20949  https://en.wikipedia.org/wiki/Harry_Potter_and...  \n",
       "23440  https://en.wikipedia.org/wiki/On_the_Job_(2013...  \n",
       "21250   https://en.wikipedia.org/wiki/Candlestick_(film)  \n",
       "25629  https://en.wikipedia.org/wiki/The_Prince_of_Light  \n",
       "26654        https://en.wikipedia.org/wiki/Simran_(film)  \n",
       "21077              https://en.wikipedia.org/wiki/Skyfall  \n",
       "22886       https://en.wikipedia.org/wiki/Paper_Marriage  \n",
       "20434  https://en.wikipedia.org/wiki/Four_Weddings_an...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchEngine1(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a new score, Search Engine 3:\n",
    "###### We did 3rd search engine with \"zone index\" methods. We get info about this metohod from this link: https://moz.com/blog/search-engine-algorithm-basics.\n",
    "\n",
    "We conseidered the following sections for every movie: title, intro, plot and music. This method consists on assigning a fixed score. In our case we have:\n",
    "* title: 0.9\n",
    "* intro: 0.4\n",
    "* plot: 0.3\n",
    "* music: 0.6\n",
    "\n",
    "Then, we search all documents that contain input query (in title/intro/plot/music).\n",
    "For every document we get its score that we calculate in the following way:\n",
    "This score is a sum of different score in every section. You can sum the score for every section if and only if:\n",
    "* score about titile: if query contains the whole title;\n",
    "* score about intro/plot: if intro/plot contains the whole query;\n",
    "* score about music: if at least one word of the query is in music section.\n",
    "\n",
    "With this scoring we should give more importance to music and title section. \n",
    "We have choosen to give this score in the previous way about music because we want to sum this score when only name or surname (or both) of music compositor is in the input query. About title, we want to give this score becuase title must be totally in the input query.\n",
    "About intro and plot, we have given these score because we think that intro contains more significant words than words in plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created index for the search engine 3 and we put it in index3.tsv file. In this index, we considered also 'music' section to match document for every word. It's similiar to index1.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "\n",
    "#create index and save it on index.tsv                \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "with open('HW3 ADM/ind/vocabulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        h=0\n",
    "        for row in tsv_vocabulary:\n",
    "            \n",
    "            dict2[row[1]]=[]\n",
    "        for index in range(len(totalMovies)):\n",
    "            \n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                music=data_list[1][8]\n",
    "                music = ast.literal_eval(music)\n",
    "                text=plot+intro+music\n",
    "                text= list(set(map(str.lower, text)))\n",
    "                \n",
    "                #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "                for i in text:\n",
    "                    for row in tsv_vocabulary:\n",
    "                        if i==row[0]:\n",
    "                            doc=\"document_\"\n",
    "                            name2=\"{}{}\".format(doc,index)\n",
    "                            \n",
    "                            dict2[row[1]].append(name2)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "        #put dict2 in index.tsv file. In. evry row we have a single term_id with occurences of respective word.\n",
    "        with open('HW3 ADM/tsv/index3.tsv', 'w', newline='') as f_output:\n",
    "            tsv_vocabulary = csv.writer(f_output, delimiter='\\t')           \n",
    "            for key, val in dict2.items():\n",
    "                tsv_vocabulary.writerow([key, val])           \n",
    "                    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function 'heapSortK' allows us to use a heap data structure (using heapq library) for maintaining the top-k documents. This function has a list and a k value. This value is the number of ordered results that we want to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import heapq\n",
    "def heappush(h, item, key=lambda x: x):\n",
    "    heapq.heappush(h, (key(item), item))\n",
    "\n",
    "def heappop(h):\n",
    "    return heapq.heappop(h)[1]\n",
    "\n",
    "def heapify(h, key=lambda x: x):\n",
    "    for idx, item in enumerate(h):\n",
    "        h[idx] = (key(item), item)\n",
    "    heapq.heapify(h)\n",
    "def heapFunction(a,k): \n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'plot', 'music','starring', 'score'])\n",
    "    result=[]\n",
    "    h = []\n",
    "    for item in a:\n",
    "        heappush(h, item, key=itemgetter(-1))\n",
    "    #print(h)\n",
    "    while h:\n",
    "        result.append(heappop(h))\n",
    "    \n",
    "    \n",
    "    j=0    #print(result)\n",
    "    result.reverse()\n",
    "    \n",
    "    for i in range(k):\n",
    "      \n",
    "        if len(a)>i:\n",
    "            \n",
    "            df.loc[j] = result[i]\n",
    "            j+=1\n",
    "        else:\n",
    "            break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 'search3' we have calculated the result for the 3rd search engine.\n",
    "Initially we have created a 'df_score' dataframe to store fixed score for every section (taht we considered). Then we call 'getDocuments(query,3)' to get all dcouments where all words in query are present on them (based on index3.tsv).\n",
    "\n",
    "For every document, we check the following aspects:\n",
    "* if all query words are in title section, sum score about title in 'df_score' to 'score' (final score);\n",
    "* if all query words are in intro section, sum score about intro in 'df_score' to 'score' (final score);\n",
    "* if all 'title_section' words are in input query, sum score about title in 'df_score' to 'score' (final score);\n",
    "* if at least one word in inout query is in 'music' section, sum score about music in 'df_score' to 'score' (final score);\n",
    "\n",
    "Final dataframe 'df' contains all movies searched with these following data (title,intro,plot,music, score). It's ordered by this score from 'heapFunction' function and it contains only first k movies. K is a value that the user has choosen. If there are less results than k, only available results are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def searchEngine3(query,k):\n",
    "    \n",
    "    document=getDocuments(query,3)\n",
    "    if document=='No results':\n",
    "        \n",
    "        return document\n",
    "    elif document==[]:\n",
    "        return \"No results\"\n",
    "    listWords = query.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'plot', 'music','starring', 'score'])\n",
    "    #Create datarame that indicates score for every section.\n",
    "    df_score=pd.DataFrame(columns=['title_score', 'intro_score', 'plot_score', 'music_score'])\n",
    "    scores=[0.8,0.4,0.3,0.6]\n",
    "    df_score.loc[0]=scores\n",
    "    resultMovies=[]\n",
    "    actors=[]\n",
    "    \n",
    "    for index in range(len(document)):\n",
    "        score=0\n",
    "        lista=[]\n",
    "        #get id of documnets_is\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        url=totalMovies[int(numberDocument)]\n",
    "        name=\"article_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open(\"HW3 ADM/tsv_correct/\"+file,\"r\") as tsvfile:\n",
    "                    tsv_index = list(csv.reader(tsvfile, delimiter='\\t'))\n",
    "                    title=ast.literal_eval(tsv_index[1][3])\n",
    "                    \n",
    "                    intro=ast.literal_eval(tsv_index[1][1])\n",
    "\n",
    "                    plot=ast.literal_eval(tsv_index[1][2])\n",
    "                    \n",
    "                    music=ast.literal_eval(tsv_index[1][8])\n",
    "                    #actors.append(tsv_index[1][7])\n",
    "                    \n",
    "                    if (all(elem in title  for elem in listWords)) or (all(elem in listWords  for elem in title)):\n",
    "                            score+=df_score.loc[0]['title_score']\n",
    "                            \n",
    "                    if all(elem in intro  for elem in listWords)==True:\n",
    "                            score+=df_score.loc[0]['intro_score']\n",
    "                            \n",
    "                    if all(elem in plot  for elem in listWords)==True:\n",
    "                            score+=df_score.loc[0]['plot_score']\n",
    "                            \n",
    "                    if any(elem in music  for elem in listWords)==True:\n",
    "                        \n",
    "                        score+=df_score.loc[0]['music_score']\n",
    "                    \n",
    "\n",
    "        with open('HW3 ADM/tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_file = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title2=tsv_file[1][3]\n",
    "            intro2=tsv_file[1][1]\n",
    "            plot2=tsv_file[1][2]\n",
    "            music2=tsv_file[1][8]\n",
    "            listActors=ast.literal_eval(tsv_file[1][7])\n",
    "            actors=listActors\n",
    "            film=[title2,intro2,plot2,music2,actors,score]\n",
    "            resultMovies.append(film)\n",
    "            \n",
    "            \n",
    "       \n",
    "    df=heapFunction(resultMovies,k)\n",
    "    actorsGraph=[]\n",
    "    for index, row in df.iterrows():\n",
    "        actorsGraph.append(row['starring'])\n",
    "    \n",
    "    graph(actorsGraph)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Step: Make a nice visualization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this following function 'graph'we did the bonus section. This function has a nested list of actor, as parameter, about every film in results of search3. With this function we calculate evry couple of actors that are in (at least) two different film through film on the search3 result. Then we show these couple in a graph wehere every couple is represented by two nodes linked with an edge. If there is no actors couple, we don't show this graph.\n",
    "We used 'networkx' python library, as suggested in hw track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "def graph(listMovies):\n",
    "    G = nx.Graph() \n",
    "    couple=[]\n",
    "    count=0\n",
    "    for i in range(len(listMovies)):\n",
    "        for j in range(len(listMovies[i])):\n",
    "            for y in range(1,len(listMovies[i])):\n",
    "                couple=[listMovies[i][j],listMovies[i][y]]\n",
    "                count=0\n",
    "                for k in listMovies:\n",
    "                    if (couple[0] in k) and (couple[1] in k):\n",
    "                        \n",
    "                        count+=1\n",
    "                        if count>=2 and not couple[0] == couple[1]:\n",
    "                            \n",
    "                            G.add_edge(couple[0],couple[1])\n",
    "                            break\n",
    "     \n",
    " \n",
    "    \n",
    "    if not nx.is_empty(G):\n",
    "        pos = nx.spring_layout(G)   #<<<<<<<<<< Initialize this only once\n",
    "        nx.draw(G,pos=pos, with_labels=True, node_size = 100, font_size=10)\n",
    "        nx.draw_networkx_nodes(G,pos=pos, with_labels=True, node_size = 1500, font_size=10)\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.3)#<<<<<<<<< pass the pos variable\n",
    "        #plt.draw() \n",
    "        plt.figure(figsize=(8, 8))  # image is 8 x 8 inches\n",
    "         # To plot the next graph in a new figure\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>plot</th>\n",
       "      <th>music</th>\n",
       "      <th>starring</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Stuff</td>\n",
       "      <td>\\nThe Stuff (also known as Larry Cohen's The S...</td>\n",
       "      <td>Several railroad workers discover a yogurt-lik...</td>\n",
       "      <td>Anthony GuefenJingles:Richard Seaman</td>\n",
       "      <td>[Michael Moriarty, Andrea Marcovicci, Garrett ...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Miles from Home</td>\n",
       "      <td>Miles from Home is a 1988 film starring Richar...</td>\n",
       "      <td>The Roberts family farm in Iowa is a prosperou...</td>\n",
       "      <td>Robert Folk</td>\n",
       "      <td>[Richard Gere, Kevin Anderson, Penelope Ann Mi...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Watchmen</td>\n",
       "      <td>\\nWatchmen is a 2009 American superhero film b...</td>\n",
       "      <td>In an alternate United States, beginning in 19...</td>\n",
       "      <td>Tyler Bates</td>\n",
       "      <td>[Malin Åkerman, Billy Crudup, Matthew Goode, C...</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rooster Cogburn</td>\n",
       "      <td>Rooster Cogburn is a 1975 American Adventure W...</td>\n",
       "      <td>This article's plot summary may be too long or...</td>\n",
       "      <td>Laurence Rosenthal</td>\n",
       "      <td>[John Wayne, Katharine Hepburn, Richard Jordan...</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canadian Bacon</td>\n",
       "      <td>\\nCanadian Bacon is a 1995 American-Canadian c...</td>\n",
       "      <td>Thousands of former employees are outraged wit...</td>\n",
       "      <td>Elmer Bernstein\\nPeter Bernstein</td>\n",
       "      <td>[Alan Alda, John Candy, Bill Nunn, Kevin J. O'...</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                              intro  \\\n",
       "0        The Stuff  \\nThe Stuff (also known as Larry Cohen's The S...   \n",
       "1  Miles from Home  Miles from Home is a 1988 film starring Richar...   \n",
       "2         Watchmen  \\nWatchmen is a 2009 American superhero film b...   \n",
       "3  Rooster Cogburn  Rooster Cogburn is a 1975 American Adventure W...   \n",
       "4   Canadian Bacon  \\nCanadian Bacon is a 1995 American-Canadian c...   \n",
       "\n",
       "                                                plot  \\\n",
       "0  Several railroad workers discover a yogurt-lik...   \n",
       "1  The Roberts family farm in Iowa is a prosperou...   \n",
       "2  In an alternate United States, beginning in 19...   \n",
       "3  This article's plot summary may be too long or...   \n",
       "4  Thousands of former employees are outraged wit...   \n",
       "\n",
       "                                  music  \\\n",
       "0  Anthony GuefenJingles:Richard Seaman   \n",
       "1                           Robert Folk   \n",
       "2                           Tyler Bates   \n",
       "3                    Laurence Rosenthal   \n",
       "4      Elmer Bernstein\\nPeter Bernstein   \n",
       "\n",
       "                                            starring  score  \n",
       "0  [Michael Moriarty, Andrea Marcovicci, Garrett ...    0.6  \n",
       "1  [Richard Gere, Kevin Anderson, Penelope Ann Mi...    0.4  \n",
       "2  [Malin Åkerman, Billy Crudup, Matthew Goode, C...    0.3  \n",
       "3  [John Wayne, Katharine Hepburn, Richard Jordan...    0.3  \n",
       "4  [Alan Alda, John Candy, Bill Nunn, Kevin J. O'...    0.3  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='United States Richard heroes'\n",
    "searchEngine3(query,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>plot</th>\n",
       "      <th>music</th>\n",
       "      <th>starring</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Moana</td>\n",
       "      <td>\\nMoana (also known as Vaiana[4] or Oceania,[5...</td>\n",
       "      <td>On the Polynesian island of Motunui, the inhab...</td>\n",
       "      <td>Songs:\\nLin-Manuel Miranda\\nOpetaia Foa'i\\nMar...</td>\n",
       "      <td>[Auliʻi Cravalho, Dwayne Johnson, Rachel House...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Rugrats Movie</td>\n",
       "      <td>\\nThe Rugrats Movie is a 1998 American animate...</td>\n",
       "      <td>Nine months after the events of The Family Tre...</td>\n",
       "      <td>Mark Mothersbaugh[1]</td>\n",
       "      <td>[E. G. Daily, Tara Strong, Christine Cavanaugh...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of Mice and Men</td>\n",
       "      <td>\\nOf Mice and Men is a 1992 American period dr...</td>\n",
       "      <td>The movie opens with George in a boxcar, remin...</td>\n",
       "      <td>Mark Isham</td>\n",
       "      <td>[John Malkovich, Gary Sinise, Casey Siemaszko]</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Night Falls on Manhattan</td>\n",
       "      <td>Night Falls on Manhattan is a 1996 American cr...</td>\n",
       "      <td>Detectives Liam Casey (Ian Holm) and Joey Alle...</td>\n",
       "      <td>Mark Isham</td>\n",
       "      <td>[Andy García, Richard Dreyfuss, Lena Olin, Ian...</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Young Victoria</td>\n",
       "      <td>\\nThe Young Victoria is a 2009 British-America...</td>\n",
       "      <td>Princess Victoria of Kent is the heir presumpt...</td>\n",
       "      <td>Ilan Eshkeri</td>\n",
       "      <td>[Emily Blunt, Rupert Friend, Miranda Richardso...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Man with Rain in His Shoes</td>\n",
       "      <td>\\nThe Man with Rain in His Shoes is a 1998 Spa...</td>\n",
       "      <td>Victor (Henshall) is an actor in London who is...</td>\n",
       "      <td>NA</td>\n",
       "      <td>[Lena Headey, Douglas Henshall, Penélope Cruz,...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Karate Kid</td>\n",
       "      <td>The Karate Kid is a 1984 American martial arts...</td>\n",
       "      <td>In 1984, Daniel LaRusso and his mother Lucille...</td>\n",
       "      <td>Bill Conti</td>\n",
       "      <td>[Ralph Macchio, Noriyuki \"Pat\" Morita, William...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Imitation Game</td>\n",
       "      <td>\\nThe Imitation Game is a 2014 American histor...</td>\n",
       "      <td>In 1951, two policemen, Nock and Staehl, inves...</td>\n",
       "      <td>Alexandre Desplat</td>\n",
       "      <td>[Benedict Cumberbatch, Keira Knightley, Matthe...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Robin Hood</td>\n",
       "      <td>\\nRobin Hood is a 2010 British-American epic h...</td>\n",
       "      <td>\\nIn the year 1199, Robin Longstride serves as...</td>\n",
       "      <td>Marc Streitenfeld</td>\n",
       "      <td>[Russell Crowe, Cate Blanchett, William Hurt, ...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kingsman: The Secret Service</td>\n",
       "      <td>\\nKingsman: The Secret Service is a 2014 actio...</td>\n",
       "      <td>During a mission in the Middle East in 1997, p...</td>\n",
       "      <td>Henry Jackman\\nMatthew Margeson</td>\n",
       "      <td>[Colin Firth, Samuel L. Jackson, Mark Strong, ...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kingsman: The Golden Circle</td>\n",
       "      <td>\\nKingsman: The Golden Circle is a 2017 action...</td>\n",
       "      <td>A year after defeating Richmond Valentine and ...</td>\n",
       "      <td>Henry Jackman\\nMatthew Margeson</td>\n",
       "      <td>[Colin Firth, Julianne Moore, Taron Egerton, M...</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "0                            Moana   \n",
       "1                The Rugrats Movie   \n",
       "2                  Of Mice and Men   \n",
       "3         Night Falls on Manhattan   \n",
       "4               The Young Victoria   \n",
       "5   The Man with Rain in His Shoes   \n",
       "6                   The Karate Kid   \n",
       "7               The Imitation Game   \n",
       "8                       Robin Hood   \n",
       "9     Kingsman: The Secret Service   \n",
       "10     Kingsman: The Golden Circle   \n",
       "\n",
       "                                                intro  \\\n",
       "0   \\nMoana (also known as Vaiana[4] or Oceania,[5...   \n",
       "1   \\nThe Rugrats Movie is a 1998 American animate...   \n",
       "2   \\nOf Mice and Men is a 1992 American period dr...   \n",
       "3   Night Falls on Manhattan is a 1996 American cr...   \n",
       "4   \\nThe Young Victoria is a 2009 British-America...   \n",
       "5   \\nThe Man with Rain in His Shoes is a 1998 Spa...   \n",
       "6   The Karate Kid is a 1984 American martial arts...   \n",
       "7   \\nThe Imitation Game is a 2014 American histor...   \n",
       "8   \\nRobin Hood is a 2010 British-American epic h...   \n",
       "9   \\nKingsman: The Secret Service is a 2014 actio...   \n",
       "10  \\nKingsman: The Golden Circle is a 2017 action...   \n",
       "\n",
       "                                                 plot  \\\n",
       "0   On the Polynesian island of Motunui, the inhab...   \n",
       "1   Nine months after the events of The Family Tre...   \n",
       "2   The movie opens with George in a boxcar, remin...   \n",
       "3   Detectives Liam Casey (Ian Holm) and Joey Alle...   \n",
       "4   Princess Victoria of Kent is the heir presumpt...   \n",
       "5   Victor (Henshall) is an actor in London who is...   \n",
       "6   In 1984, Daniel LaRusso and his mother Lucille...   \n",
       "7   In 1951, two policemen, Nock and Staehl, inves...   \n",
       "8   \\nIn the year 1199, Robin Longstride serves as...   \n",
       "9   During a mission in the Middle East in 1997, p...   \n",
       "10  A year after defeating Richmond Valentine and ...   \n",
       "\n",
       "                                                music  \\\n",
       "0   Songs:\\nLin-Manuel Miranda\\nOpetaia Foa'i\\nMar...   \n",
       "1                                Mark Mothersbaugh[1]   \n",
       "2                                          Mark Isham   \n",
       "3                                          Mark Isham   \n",
       "4                                        Ilan Eshkeri   \n",
       "5                                                  NA   \n",
       "6                                          Bill Conti   \n",
       "7                                   Alexandre Desplat   \n",
       "8                                   Marc Streitenfeld   \n",
       "9                     Henry Jackman\\nMatthew Margeson   \n",
       "10                    Henry Jackman\\nMatthew Margeson   \n",
       "\n",
       "                                             starring  score  \n",
       "0   [Auliʻi Cravalho, Dwayne Johnson, Rachel House...    1.0  \n",
       "1   [E. G. Daily, Tara Strong, Christine Cavanaugh...    0.6  \n",
       "2      [John Malkovich, Gary Sinise, Casey Siemaszko]    0.6  \n",
       "3   [Andy García, Richard Dreyfuss, Lena Olin, Ian...    0.6  \n",
       "4   [Emily Blunt, Rupert Friend, Miranda Richardso...    0.4  \n",
       "5   [Lena Headey, Douglas Henshall, Penélope Cruz,...    0.4  \n",
       "6   [Ralph Macchio, Noriyuki \"Pat\" Morita, William...    0.4  \n",
       "7   [Benedict Cumberbatch, Keira Knightley, Matthe...    0.4  \n",
       "8   [Russell Crowe, Cate Blanchett, William Hurt, ...    0.4  \n",
       "9   [Colin Firth, Samuel L. Jackson, Mark Strong, ...    0.4  \n",
       "10  [Colin Firth, Julianne Moore, Taron Egerton, M...    0.4  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='United States Strong Mark '\n",
    "searchEngine3(query,11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
