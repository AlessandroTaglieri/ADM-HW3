{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tsv files in 'tsv_correct2' directory where we have preprocessed the tsv file (just created in parser.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from shutil import move\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "name=\"aritcle_\"\n",
    "extension2=\".tsv\"\n",
    "exclude = string.punctuation\n",
    "for index in range(len(listUrl_Movies3)):\n",
    "\n",
    "\n",
    "\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"tsv/\"+file,\"r\") as tsvfile, open(\"tsv_correct2/\"+file,\"w\") as outfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        tsvwriter = csv.writer(outfile, delimiter=\"\\t\")\n",
    "        for row in tsvreader:\n",
    "            for i in range(len(row)):\n",
    "                #take every words, deleting ountuaction and other symbols\n",
    "                row[i] = tokenizer.tokenize(row[i])\n",
    "                #remove duplicate case-insensitive elements\n",
    "                row[i]= list(map(str.lower, row[i]))\n",
    "                #row[i] = row[i].translate({ord(c): None for c in string.punctuation})\n",
    "                \n",
    "            tsvwriter.writerow(row)\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dataframe that match tfidf bettween every document and every word  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             \n",
    "\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "documents=[]\n",
    "name='aritcle_'\n",
    "extension2='.tsv'\n",
    "with open('tsv/vocobulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for index in range(0,10000):\n",
    "            print(index)\n",
    "            file=\"{}{}{}\".format(name,index,extension2)\n",
    "            with open(\"tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "                data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "                tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "                intro=data_list[1][1]\n",
    "                intro = ast.literal_eval(intro)\n",
    "                plot=data_list[1][2]\n",
    "                plot = ast.literal_eval(plot)\n",
    "                text=plot+intro\n",
    "                text=list(map(str.lower, text))\n",
    "                documents.append(' '.join(text))\n",
    "                #print(documents)\n",
    "                # text2= list(set(map(str.lower, text)))\n",
    "                \n",
    "                \n",
    "        vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "        vectors = vectorizer.fit_transform(documents)\n",
    "        #print(vectors)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        df2 = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create index.tsv with iditf from preovious dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "        \n",
    "import ast\n",
    "from itertools import islice\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "#create index and save it on index.tsv                \n",
    "dict={}\n",
    "dict2 = {}\n",
    "count=0\n",
    "present=False\n",
    "\n",
    "\n",
    "print('step 2')\n",
    "name=\"aritcle_\"\n",
    "extension2=\".tsv\"\n",
    "h=0\n",
    "for row in tsv_vocabulary:\n",
    "    h+=1\n",
    "    dict[row[0]]=row[1]\n",
    "    print(h)\n",
    "    dict2[row[1]]=[]\n",
    "for index in range(0,10000):\n",
    "    print(\"numero documento \"+ str(index))\n",
    "    file=\"{}{}{}\".format(name,index,extension2)\n",
    "    with open(\"tsv_correct2/\"+file,\"r\") as tsvfile:\n",
    "        data_list = list(csv.reader(tsvfile, delimiter=\"\\t\"))\n",
    "        tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        intro=data_list[1][1]\n",
    "        intro = ast.literal_eval(intro)\n",
    "        plot=data_list[1][2]\n",
    "        plot = ast.literal_eval(plot)\n",
    "        text=plot+intro\n",
    "        text=list(map(str.lower, text))\n",
    "    \n",
    "        text2= list(set(map(str.lower, text)))\n",
    "        #print(text2)\n",
    "                        #for evry words in plot adn intro (for every page) we get every word. From every word we get its term_id and put it whit their occurences (document_id) in dict2\n",
    "        for i in text2:\n",
    "\n",
    "                            #print(\"word   \"+str(i))\n",
    "            # print(\"word \"+str(i))\n",
    "            # print(\"aaa\" + str(df2.iloc[index][i]))\n",
    "            res=df2.iloc[index][i]\n",
    "                            #print(\"res   \"+str(res))\n",
    "            for term in dict:\n",
    "                if i==term:\n",
    "                    print(\"key \"+ str(term))\n",
    "                    print(\"value \"+ str(dict[term]))\n",
    "                    doc=\"document_\"\n",
    "                    name2=\"{}{}\".format(doc,index)\n",
    "                    result=[name2,res]\n",
    "                    dict2[dict[term]].append(result)\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            \n",
    "\n",
    "with open('tsv/index.tsv', 'w', newline='') as f_output:\n",
    "    tsv_index2 = csv.writer(f_output, delimiter='\\t')           \n",
    "    for key, val in dict2.items():\n",
    "        tsv_index2.writerow([key, val])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions: to get itidf about query, to get itidf about documnet_id for wirds in query, to calculate coisine similiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def getTfidf_query(query):\n",
    "\n",
    "\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "      \n",
    "    vectors = vectorizer.fit_transform([query])\n",
    "        #print(vectors)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df_query = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return df_query\n",
    "\n",
    "\n",
    "#function \n",
    "def getTfidf_document(words,document_id):\n",
    "    dict={}\n",
    "    \n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "    #print(listWords)\n",
    "    df=pd.DataFrame(columns=listWords)\n",
    "    tfIdf=[]\n",
    "    with open('tsv/index.tsv', 'r', newline='') as f_output:\n",
    "        tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        \n",
    "    for word in listWords:\n",
    "        listDoc=[]\n",
    "        #print(\"word \" + word)\n",
    "        for row in tsv_vocabulary:\n",
    "            if word.lower()==row[0]:\n",
    "                term=row[1]\n",
    "                #print(\"teerm\" + str(term))\n",
    "                break\n",
    "        for row in tsv_index:\n",
    "            if term==row[0]:\n",
    "                \n",
    "                listDoc=ast.literal_eval(row[1])\n",
    "                \n",
    "                #print(listDoc)\n",
    "                break\n",
    "        for index in listDoc:\n",
    "            \n",
    "            #print(index)\n",
    "            if index[0]==document_id:\n",
    "                #print(index[0])\n",
    "                \n",
    "                tfIdf.append(index[1])\n",
    "                #print(index[1])\n",
    "                break\n",
    "        \n",
    "    df.loc[0]=tfIdf\n",
    "    df=df.reindex(sorted(df.columns), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def coisine(list_query,list_document):\n",
    "    \n",
    "    res=(cosine_similarity([list_query,list_document]))\n",
    "    #print(res)\n",
    "    return res[0][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get documents that contain query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import ast\n",
    "#define function that allows us to calculate a list that is an intersection from two list\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "def getDocuments(words):\n",
    "\n",
    "\n",
    "    #we use dict3 to store term_id and its respective documents_id\n",
    "    dict3={}\n",
    "    ##we use dict4 to store evry word and its respective documents_id\n",
    "    dict4={}\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    #in listWords we have a list that contains all words about inout query\n",
    "    listWords = words.split()\n",
    "    listWords=[x.lower() for x in listWords]\n",
    "\n",
    "    #with vocabulary.tsv we start to build a dict3 with term_id for every words in wordsList\n",
    "    with open('HW3 ADM/tsv/vocobulary.tsv', 'r', newline='') as f_output:\n",
    "        tsv_vocabulary = list(csv.reader(f_output, delimiter='\\t'))\n",
    "        for word in listWords:\n",
    "            word=word.lower()\n",
    "            present=False\n",
    "            for row in tsv_vocabulary:\n",
    "                if word.lower()==row[0]:\n",
    "                    dict3[row[1]]=[]\n",
    "                    present=True\n",
    "            #case where word is not in vocabulary\n",
    "            if present==False:\n",
    "                dict4[word]=[]\n",
    "\n",
    "        #we continue to match dicumnets_id to every term_id in dict3\n",
    "        with open('HW3 ADM/tsv/index2.tsv', 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            for k in dict3.keys():  \n",
    "                for row in tsv_index:\n",
    "                    if row[0]==k:\n",
    "                        dict3[k]=row[1]\n",
    "                        continue\n",
    "\n",
    "\n",
    "        #finally we build dict4 where evry word matches to respective documents_id\n",
    "        for k in dict3.keys():\n",
    "\n",
    "            for row in tsv_vocabulary:\n",
    "                if k==row[1]:\n",
    "                    dict4[row[0]]=dict3[row[1]]\n",
    "\n",
    "        document=ast.literal_eval(dict4[listWords[0]])         \n",
    "        #interection between every list in values dict4. In this way we have documnets_id where all words (in query input) are present\n",
    "        for value in dict4.values():\n",
    "            document=intersection(document,ast.literal_eval(value))\n",
    "        #print(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function that execute searchEngine2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def searchEngine2(words):\n",
    "    #build the dataframe with info for every documents_id\n",
    "    document=getDocuments(words)\n",
    "    df_query=getTfidf_query(words)\n",
    "    df=pd.DataFrame(columns=['title', 'intro', 'url','similarity'])\n",
    "\n",
    "    for index in range(len(document)):\n",
    "        #get id of documnets_is\n",
    "        df_document=getTfidf_document(words,document[index])\n",
    "        #print(df_document)\n",
    "        list_query=list(df_query.loc[0])\n",
    "        #print(list_query)\n",
    "        list_document=list(df_document.loc[0])\n",
    "        #print(list_document)\n",
    "        similiarity=coisine(list_document,list_query)\n",
    "        #print(\"simi \"+ str(similiarity))\n",
    "        numberDocument=document[index][9:]\n",
    "        #get wikipedia url\n",
    "        url=listUrl_Movies3[int(numberDocument)]\n",
    "        name=\"aritcle_\"\n",
    "        extension2=\".tsv\"\n",
    "        index=int(numberDocument)\n",
    "        file=\"{}{}{}\".format(name,index,extension2)\n",
    "        #get info about title and intro for evert film that corresponds to every documents_id\n",
    "        with open('tsv/'+file, 'r', newline='') as f_output:\n",
    "            tsv_index = list(csv.reader(f_output, delimiter='\\t'))\n",
    "            title=tsv_index[1][3]\n",
    "            intro=tsv_index[1][1]\n",
    "            film=[title,intro,url,similiarity]\n",
    "            #put all info for every film in a single row of df dataframe\n",
    "            df.loc[index] = film\n",
    "            df=df.sort_values(by=['similarity'],ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execute searchEngine2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "query='life 2019 horror story'\n",
    "\n",
    "searchEngine2(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
